From fbf79e3eed99133315836a6b5b7ae5e370c1fc6e Mon Sep 17 00:00:00 2001
From: Phantom X <PhantomX@users.noreply.github.com>
Date: Sun, 30 Jan 2022 11:09:04 -0300
Subject: [PATCH] Revert SSE4 updates

Rebased with commits reverted or manually rebased:
8a9ec4c7067630c25140cd066cc6a40eac83a0c3 core: purge sse2
48f51b3ce3da5ce7dc64ea87dc720ec62936f68f gs: purge sse2/sse3
6bc31a00237798a755ff7de4311459ff3f318ffd cmake: make sse4 default
f9d96f55a538ce373f1662a5554d8052173940b1 microVU: Remove SSE4 op from Overflow flag checks + clean up
ee07f860fc1dd7e8ada6c13903bb3a592b8d4864 microVU: Implement Overflow checks
d0f3c620d27f88c0eabdbb60d7737284c239909e GSBlock: Cleanup warnings.
2dbfe9f743d795da0597ff29b302541132a48c6c gs: Cleanup remaining stuff from sse2/3 purge.
7faa5db9e57c6e59dbe62c66d64623d879b680a2 VU/GameDB: Move Mac/Status overflow flag checks to a gamefix
2e1d147135a5b05d59095afd30d1ee2e0918b36d GS: Faster accurate_stq calculations
bd8fcc8f81242c8916a41509f76b6626b4720f50 GS: Remove inaccurate stq calculations from GSVertexTrace
---
 bin/resources/GameIndex.yaml                  |  46 ++-
 cmake/BuildParameters.cmake                   |  10 +-
 pcsx2/Config.h                                |   2 -
 pcsx2/GS/GSBlock.h                            | 339 +++++++++++++++++-
 pcsx2/GS/GSCapture.cpp                        |  10 +
 pcsx2/GS/GSState.cpp                          |  11 +
 pcsx2/GS/GSUtil.cpp                           |   6 +
 pcsx2/GS/GSVector4.h                          |  61 +++-
 pcsx2/GS/GSVector4i.h                         | 205 ++++++++++-
 pcsx2/GS/Renderers/Common/GSVertexTrace.cpp   |  19 +
 pcsx2/GS/Renderers/HW/GSRendererNew.cpp       |   4 +
 .../SW/GSDrawScanlineCodeGenerator.all.cpp    |  96 ++++-
 pcsx2/GS/Renderers/SW/GSRendererSW.cpp        |  20 ++
 pcsx2/PCSX2Base.h                             |   8 +-
 pcsx2/Pcsx2Config.cpp                         |   7 -
 pcsx2/gui/AppInit.cpp                         |  11 +-
 pcsx2/gui/Panels/GameFixesPanel.cpp           |   4 -
 pcsx2/x86/iMMI.cpp                            | 253 +++++++++++--
 pcsx2/x86/ix86-32/iR5900-32.cpp               |   4 +-
 pcsx2/x86/microVU.cpp                         |   4 +-
 pcsx2/x86/microVU_Alloc.inl                   |  11 +-
 pcsx2/x86/microVU_Clamp.inl                   |  32 +-
 pcsx2/x86/microVU_Lower.inl                   |  23 +-
 pcsx2/x86/microVU_Misc.inl                    | 138 ++++++-
 pcsx2/x86/microVU_Upper.inl                   |  34 +-
 pcsx2/x86/newVif_UnpackSSE.cpp                | 110 +++++-
 26 files changed, 1310 insertions(+), 158 deletions(-)

diff --git a/bin/resources/GameIndex.yaml b/bin/resources/GameIndex.yaml
index dec6b75..920bdc3 100644
--- a/bin/resources/GameIndex.yaml
+++ b/bin/resources/GameIndex.yaml
@@ -14863,8 +14863,15 @@ SLES-53744:
 SLES-53746:
   name: "Superman Returns"
   region: "PAL-E"
-  gameFixes:
-    - VUOverflowHack # Fixes SPS.
+  patches:
+    E8F7BAB6:
+      content: |-
+        author=kozarovv
+        // Fix sps and various graphical issues by Using iaddiu instead of FSAND
+        // require preload frame data and crc hack atleast minimum for GSDX HW
+        patch=1,EE,00639Ef0,word,10050208
+        patch=1,EE,0063a068,word,10080208
+        patch=1,EE,0063D488,word,10020208
 SLES-53747:
   name: "Ed, Edd, 'n Eddy - The Misadventure"
   region: "PAL-E"
@@ -16076,23 +16083,31 @@ SLES-54347:
 SLES-54348:
   name: "Superman Returns"
   region: "PAL-F"
-  gameFixes:
-    - VUOverflowHack # Fixes SPS.
 SLES-54349:
   name: "Superman Returns"
   region: "PAL-I"
-  gameFixes:
-    - VUOverflowHack # Fixes SPS.
+  patches:
+    E7F7B6BD:
+      content: |-
+        author=kozarovv
+        // Use iaddiu instead of FSAND.
+        patch=1,EE,00639ef0,word,10050208
+        patch=1,EE,0063a068,word,10080208
 SLES-54350:
   name: "Superman Returns"
   region: "PAL-G"
-  gameFixes:
-    - VUOverflowHack # Fixes SPS.
 SLES-54351:
   name: "Superman Returns"
   region: "PAL-S"
-  gameFixes:
-    - VUOverflowHack # Fixes SPS.
+  patches:
+    EDF0A0A7:
+      content: |-
+        author=kozarovv
+        // Fix sps and various graphical issues by Using iaddiu instead of FSAND
+        // require preload frame data and crc hack atleast minimum for GSDX HW
+        patch=1,EE,00639Ef0,word,10050208
+        patch=1,EE,0063a068,word,10080208
+        patch=1,EE,0063D488,word,10020208
 SLES-54354:
   name: "Final Fantasy XII"
   region: "PAL-E"
@@ -39329,8 +39344,15 @@ SLUS-21434:
   name: "Superman Returns - The Video Game"
   region: "NTSC-U"
   compat: 4
-  gameFixes:
-    - VUOverflowHack # Fixes SPS.
+  patches:
+    E1BF5DCA:
+      content: |-
+        author=kozarovv
+        // Fix sps and various graphical issues by Using iaddiu instead of FSAND
+        // require preload frame data and crc hack atleast minimum for GSDX HW
+        patch=1,EE,00639E70,word,10050208
+        patch=1,EE,00639FE8,word,10080208
+        patch=1,EE,0063D408,word,10020208
 SLUS-21435:
   name: "One Piece - Grand Adventure"
   region: "NTSC-U"
diff --git a/cmake/BuildParameters.cmake b/cmake/BuildParameters.cmake
index bb61086..f678508 100644
--- a/cmake/BuildParameters.cmake
+++ b/cmake/BuildParameters.cmake
@@ -125,7 +125,7 @@ option(CMAKE_BUILD_PO "Build po files (modifies git-tracked files)" OFF)
 #-------------------------------------------------------------------------------
 # Select the architecture
 #-------------------------------------------------------------------------------
-option(DISABLE_ADVANCE_SIMD "Disable advance use of SIMD (SSE2+ & AVX)" OFF)
+option(DISABLE_ADVANCE_SIMD "Disable advance use of SIMD (SSE2)" OFF)
 
 # Print if we are cross compiling.
 if(CMAKE_CROSSCOMPILING)
@@ -157,9 +157,9 @@ if(${PCSX2_TARGET_ARCHITECTURES} MATCHES "i386")
 		else()
 			if (DISABLE_ADVANCE_SIMD)
 				if (USE_ICC)
-					set(ARCH_FLAG "-msse2 -msse4.1")
+					set(ARCH_FLAG "-msse2")
 				else()
-					set(ARCH_FLAG "-msse -msse2 -msse4.1 -mfxsr -march=i686")
+					set(ARCH_FLAG "-msse -msse2 -mfxsr -march=i686")
 				endif()
 			else()
 				# AVX requires some fix of the ABI (mangling) (default 2)
@@ -181,9 +181,9 @@ elseif(${PCSX2_TARGET_ARCHITECTURES} MATCHES "x86_64")
 	if(NOT DEFINED ARCH_FLAG AND NOT MSVC)
 		if (DISABLE_ADVANCE_SIMD)
 			if (USE_ICC)
-				set(ARCH_FLAG "-msse2 -msse4.1")
+				set(ARCH_FLAG "-msse2")
 			else()
-				set(ARCH_FLAG "-msse -msse2 -msse4.1 -mfxsr")
+				set(ARCH_FLAG "-msse -msse2 -mfxsr")
 			endif()
 		else()
 			#set(ARCH_FLAG "-march=native -fabi-version=6")
diff --git a/pcsx2/Config.h b/pcsx2/Config.h
index 9e44065..8141806 100644
--- a/pcsx2/Config.h
+++ b/pcsx2/Config.h
@@ -639,7 +639,6 @@ struct Pcsx2Config
 			VuAddSubHack : 1, // Tri-ace games, they use an encryption algorithm that requires VU ADDI opcode to be bit-accurate.
 			IbitHack : 1, // I bit hack. Needed to stop constant VU recompilation in some games
 			VUKickstartHack : 1, // Gives new VU programs a slight head start and runs VU's ahead of EE to avoid VU register reading/writing issues
-			VUOverflowHack : 1, // Tries to simulate overflow flag checks (not really possible on x86 without soft floats)
 			XgKickHack : 1; // Erementar Gerad, adds more delay to VU XGkick instructions. Corrects the color of some graphics, but breaks Tri-ace games and others.
 		BITFIELD_END
 
@@ -910,7 +909,6 @@ namespace EmuFolders
 #define CHECK_VIFFIFOHACK (EmuConfig.Gamefixes.VIFFIFOHack) // Pretends to fill the non-existant VIF FIFO Buffer.
 #define CHECK_VIF1STALLHACK (EmuConfig.Gamefixes.VIF1StallHack) // Like above, processes FIFO data before the stall is allowed (to make sure data goes over).
 #define CHECK_GIFFIFOHACK (EmuConfig.Gamefixes.GIFFIFOHack) // Enabled the GIF FIFO (more correct but slower)
-#define CHECK_VUOVERFLOWHACK (EmuConfig.Gamefixes.VUOverflowHack) // Special Fix for Superman Returns, they check for overflows on PS2 floats which we can't do without soft floats.
 
 //------------ Advanced Options!!! ---------------
 #define CHECK_VU_OVERFLOW (EmuConfig.Cpu.Recompiler.vuOverflow)
diff --git a/pcsx2/GS/GSBlock.h b/pcsx2/GS/GSBlock.h
index f9f548f..6ac3788 100644
--- a/pcsx2/GS/GSBlock.h
+++ b/pcsx2/GS/GSBlock.h
@@ -153,6 +153,8 @@ public:
 		{
 			GSVector4i v4((int)mask);
 
+#if _M_SSE >= 0x401
+
 			if (mask == 0xff000000 || mask == 0x00ffffff)
 			{
 				((GSVector4i*)dst)[i * 4 + 0] = ((GSVector4i*)dst)[i * 4 + 0].blend8(v0, v4);
@@ -162,11 +164,14 @@ public:
 			}
 			else
 			{
+#endif
 				((GSVector4i*)dst)[i * 4 + 0] = ((GSVector4i*)dst)[i * 4 + 0].blend(v0, v4);
 				((GSVector4i*)dst)[i * 4 + 1] = ((GSVector4i*)dst)[i * 4 + 1].blend(v1, v4);
 				((GSVector4i*)dst)[i * 4 + 2] = ((GSVector4i*)dst)[i * 4 + 2].blend(v2, v4);
 				((GSVector4i*)dst)[i * 4 + 3] = ((GSVector4i*)dst)[i * 4 + 3].blend(v3, v4);
+#if _M_SSE >= 0x401
 			}
+#endif
 		}
 
 #endif
@@ -502,7 +507,7 @@ public:
 		GSVector8i::store<true>(&dst[dstpitch * 0], v0);
 		GSVector8i::store<true>(&dst[dstpitch * 1], v1);
 
-#else
+#elif _M_SSE >= 0x301
 
 		const GSVector4i* s = (const GSVector4i*)src;
 
@@ -522,6 +527,29 @@ public:
 		GSVector4i::store<true>(&d1[0], v1);
 		GSVector4i::store<true>(&d1[1], v3);
 
+#else
+
+		const GSVector4i* s = (const GSVector4i*)src;
+
+		GSVector4i v0 = s[i * 4 + 0];
+		GSVector4i v1 = s[i * 4 + 1];
+		GSVector4i v2 = s[i * 4 + 2];
+		GSVector4i v3 = s[i * 4 + 3];
+
+		//for(int16 i = 0; i < 8; i++) {v0.i16[i] = i; v1.i16[i] = i + 8; v2.i16[i] = i + 16; v3.i16[i] = i + 24;}
+
+		GSVector4i::sw16(v0, v1, v2, v3);
+		GSVector4i::sw32(v0, v1, v2, v3);
+		GSVector4i::sw16(v0, v2, v1, v3);
+
+		GSVector4i* d0 = (GSVector4i*)&dst[dstpitch * 0];
+		GSVector4i* d1 = (GSVector4i*)&dst[dstpitch * 1];
+
+		GSVector4i::store<true>(&d0[0], v0);
+		GSVector4i::store<true>(&d0[1], v1);
+		GSVector4i::store<true>(&d1[0], v2);
+		GSVector4i::store<true>(&d1[1], v3);
+
 #endif
 	}
 
@@ -555,7 +583,7 @@ public:
 
 		// TODO: not sure if this is worth it, not in this form, there should be a shorter path
 
-#else
+#elif _M_SSE >= 0x301
 
 		const GSVector4i* s = (const GSVector4i*)src;
 
@@ -589,6 +617,36 @@ public:
 		GSVector4i::store<true>(&dst[dstpitch * 2], v1);
 		GSVector4i::store<true>(&dst[dstpitch * 3], v2);
 
+#else
+
+		const GSVector4i* s = (const GSVector4i*)src;
+
+		GSVector4i v0 = s[i * 4 + 0];
+		GSVector4i v1 = s[i * 4 + 1];
+		GSVector4i v2 = s[i * 4 + 2];
+		GSVector4i v3 = s[i * 4 + 3];
+
+		GSVector4i::sw8(v0, v1, v2, v3);
+		GSVector4i::sw16(v0, v1, v2, v3);
+		GSVector4i::sw8(v0, v2, v1, v3);
+		GSVector4i::sw64(v0, v1, v2, v3);
+
+		if((i & 1) == 0)
+		{
+			v2 = v2.yxwz();
+			v3 = v3.yxwz();
+		}
+		else
+		{
+			v0 = v0.yxwz();
+			v1 = v1.yxwz();
+		}
+
+		GSVector4i::store<true>(&dst[dstpitch * 0], v0);
+		GSVector4i::store<true>(&dst[dstpitch * 1], v1);
+		GSVector4i::store<true>(&dst[dstpitch * 2], v2);
+		GSVector4i::store<true>(&dst[dstpitch * 3], v3);
+
 #endif
 	}
 
@@ -597,6 +655,8 @@ public:
 	{
 		//printf("ReadColumn4\n");
 
+#if _M_SSE >= 0x301
+
 		const GSVector4i* s = (const GSVector4i*)src;
 
 		GSVector4i v0 = s[i * 4 + 0].xzyw();
@@ -626,6 +686,46 @@ public:
 		GSVector4i::store<true>(&dst[dstpitch * 1], v1);
 		GSVector4i::store<true>(&dst[dstpitch * 2], v2);
 		GSVector4i::store<true>(&dst[dstpitch * 3], v3);
+
+#else
+
+		const GSVector4i* s = (const GSVector4i*)src;
+
+		GSVector4i v0 = s[i * 4 + 0];
+		GSVector4i v1 = s[i * 4 + 1];
+		GSVector4i v2 = s[i * 4 + 2];
+		GSVector4i v3 = s[i * 4 + 3];
+
+		GSVector4i::sw32(v0, v1, v2, v3);
+		GSVector4i::sw32(v0, v1, v2, v3);
+		GSVector4i::sw4(v0, v2, v1, v3);
+		GSVector4i::sw8(v0, v1, v2, v3);
+		GSVector4i::sw16(v0, v2, v1, v3);
+
+		v0 = v0.xzyw();
+		v1 = v1.xzyw();
+		v2 = v2.xzyw();
+		v3 = v3.xzyw();
+
+		GSVector4i::sw64(v0, v1, v2, v3);
+
+		if((i & 1) == 0)
+		{
+			v2 = v2.yxwzlh();
+			v3 = v3.yxwzlh();
+		}
+		else
+		{
+			v0 = v0.yxwzlh();
+			v1 = v1.yxwzlh();
+		}
+
+		GSVector4i::store<true>(&dst[dstpitch * 0], v0);
+		GSVector4i::store<true>(&dst[dstpitch * 1], v1);
+		GSVector4i::store<true>(&dst[dstpitch * 2], v2);
+		GSVector4i::store<true>(&dst[dstpitch * 3], v3);
+
+#endif
 	}
 
 	static void ReadColumn32(int y, const u8* RESTRICT src, u8* RESTRICT dst, int dstpitch)
@@ -1148,13 +1248,19 @@ public:
 	{
 		for (int j = 0; j < 8; j++, dst += dstpitch)
 		{
-
+#if _M_SSE >= 0x401
 			const GSVector4i* s = (const GSVector4i*)src;
 
 			GSVector4i v0 = (s[j * 2 + 0] >> 24).gather32_32<>(pal);
 			GSVector4i v1 = (s[j * 2 + 1] >> 24).gather32_32<>(pal);
 
 			((GSVector4i*)dst)[0] = v0.pu32(v1);
+#else
+			for(int i = 0; i < 8; i++)
+			{
+				((u16*)dst)[i] = (u16)pal[src[j * 8 + i] >> 24];
+			}
+#endif
 		}
 	}
 
@@ -1173,12 +1279,19 @@ public:
 	{
 		for (int j = 0; j < 8; j++, dst += dstpitch)
 		{
+#if _M_SSE >= 0x401
 			const GSVector4i* s = (const GSVector4i*)src;
 
 			GSVector4i v0 = ((s[j * 2 + 0] >> 24) & 0xf).gather32_32<>(pal);
 			GSVector4i v1 = ((s[j * 2 + 1] >> 24) & 0xf).gather32_32<>(pal);
 
 			((GSVector4i*)dst)[0] = v0.pu32(v1);
+#else
+			for(int i = 0; i < 8; i++)
+			{
+				((u16*)dst)[i] = (u16)pal[(src[j * 8 + i] >> 24) & 0xf];
+			}
+#endif
 		}
 	}
 
@@ -1197,12 +1310,19 @@ public:
 	{
 		for (int j = 0; j < 8; j++, dst += dstpitch)
 		{
+#if _M_SSE >= 0x401
 			const GSVector4i* s = (const GSVector4i*)src;
 
 			GSVector4i v0 = (s[j * 2 + 0] >> 28).gather32_32<>(pal);
 			GSVector4i v1 = (s[j * 2 + 1] >> 28).gather32_32<>(pal);
 
 			((GSVector4i*)dst)[0] = v0.pu32(v1);
+#else
+			for(int i = 0; i < 8; i++)
+			{
+				((u16*)dst)[i] = (u16)pal[src[j * 8 + i] >> 28];
+			}
+#endif
 		}
 	}
 
@@ -1340,7 +1460,7 @@ public:
 		((GSVector8i*)dst)[6] = ((GSVector8i*)dst)[6].blend8(v2, mask);
 		((GSVector8i*)dst)[7] = ((GSVector8i*)dst)[7].blend8(v3, mask);
 
-#else
+#elif _M_SSE >= 0x301
 
 		GSVector4i v0, v1, v2, v3, v4;
 		GSVector4i mask = GSVector4i::xff000000();
@@ -1364,6 +1484,32 @@ public:
 			((GSVector4i*)dst)[i * 4 + 3] = ((GSVector4i*)dst)[i * 4 + 3].blend8(v3, mask);
 		}
 
+#else
+
+		GSVector4i v0, v1, v2, v3, v4, v5, v6, v7;
+		GSVector4i mask = GSVector4i::xff000000();
+
+		for(int i = 0; i < 4; i++, src += srcpitch * 2)
+		{
+			v4 = GSVector4i::loadl(&src[srcpitch * 0]);
+			v5 = GSVector4i::loadl(&src[srcpitch * 1]);
+
+			v6 = v4.upl16(v5);
+
+			v4 = v6.upl8(v6);
+			v5 = v6.uph8(v6);
+
+			v0 = v4.upl16(v4);
+			v1 = v4.uph16(v4);
+			v2 = v5.upl16(v5);
+			v3 = v5.uph16(v5);
+			
+			((GSVector4i*)dst)[i * 4 + 0] = ((GSVector4i*)dst)[i * 4 + 0].blend8(v0, mask);
+			((GSVector4i*)dst)[i * 4 + 1] = ((GSVector4i*)dst)[i * 4 + 1].blend8(v1, mask);
+			((GSVector4i*)dst)[i * 4 + 2] = ((GSVector4i*)dst)[i * 4 + 2].blend8(v2, mask);
+			((GSVector4i*)dst)[i * 4 + 3] = ((GSVector4i*)dst)[i * 4 + 3].blend8(v3, mask);
+		}
+
 #endif
 	}
 
@@ -1423,7 +1569,7 @@ public:
 		((GSVector8i*)dst)[6] = ((GSVector8i*)dst)[6].blend(v2, mask);
 		((GSVector8i*)dst)[7] = ((GSVector8i*)dst)[7].blend(v3, mask);
 
-#else
+#elif _M_SSE >= 0x301
 
 		GSVector4i v0, v1, v2, v3, v4, v5;
 		GSVector4i mask = GSVector4i(0x0f000000);
@@ -1460,6 +1606,47 @@ public:
 			((GSVector4i*)dst)[i * 8 + 7] = ((GSVector4i*)dst)[i * 8 + 7].blend(v3, mask);
 		}
 
+#else
+
+		GSVector4i v0, v1, v2, v3, v4, v5, v6, v7;
+		GSVector4i mask = GSVector4i(0x0f000000);
+
+		for(int i = 0; i < 2; i++, src += srcpitch * 4)
+		{
+			GSVector4i v(*(u32*)&src[srcpitch * 0], *(u32*)&src[srcpitch * 2], *(u32*)&src[srcpitch * 1], *(u32*)&src[srcpitch * 3]);
+
+			v4 = v.upl8(v >> 4);
+			v5 = v.uph8(v >> 4);
+
+			v6 = v4.upl16(v5);
+			v7 = v4.uph16(v5);
+
+			v4 = v6.upl8(v6);
+			v5 = v6.uph8(v6);
+			v6 = v7.upl8(v7);
+			v7 = v7.uph8(v7);
+
+			v0 = v4.upl16(v4);
+			v1 = v4.uph16(v4);
+			v2 = v5.upl16(v5);
+			v3 = v5.uph16(v5);
+
+			((GSVector4i*)dst)[i * 8 + 0] = ((GSVector4i*)dst)[i * 8 + 0].blend(v0, mask);
+			((GSVector4i*)dst)[i * 8 + 1] = ((GSVector4i*)dst)[i * 8 + 1].blend(v1, mask);
+			((GSVector4i*)dst)[i * 8 + 2] = ((GSVector4i*)dst)[i * 8 + 2].blend(v2, mask);
+			((GSVector4i*)dst)[i * 8 + 3] = ((GSVector4i*)dst)[i * 8 + 3].blend(v3, mask);
+
+			v0 = v6.upl16(v6);
+			v1 = v6.uph16(v6);
+			v2 = v7.upl16(v7);
+			v3 = v7.uph16(v7);
+
+			((GSVector4i*)dst)[i * 8 + 4] = ((GSVector4i*)dst)[i * 8 + 4].blend(v0, mask);
+			((GSVector4i*)dst)[i * 8 + 5] = ((GSVector4i*)dst)[i * 8 + 5].blend(v1, mask);
+			((GSVector4i*)dst)[i * 8 + 6] = ((GSVector4i*)dst)[i * 8 + 6].blend(v2, mask);
+			((GSVector4i*)dst)[i * 8 + 7] = ((GSVector4i*)dst)[i * 8 + 7].blend(v3, mask);
+		}
+
 #endif
 	}
 
@@ -1510,7 +1697,7 @@ public:
 		((GSVector8i*)dst)[6] = ((GSVector8i*)dst)[6].blend(v2, mask);
 		((GSVector8i*)dst)[7] = ((GSVector8i*)dst)[7].blend(v3, mask);
 
-#else
+#elif _M_SSE >= 0x301
 
 		GSVector4i v0, v1, v2, v3, v4, v5;
 		GSVector4i mask = GSVector4i::xf0000000();
@@ -1547,6 +1734,47 @@ public:
 			((GSVector4i*)dst)[i * 8 + 7] = ((GSVector4i*)dst)[i * 8 + 7].blend(v3, mask);
 		}
 
+#else
+
+		GSVector4i v0, v1, v2, v3, v4, v5, v6, v7;
+		GSVector4i mask = GSVector4i::xf0000000();
+
+		for(int i = 0; i < 2; i++, src += srcpitch * 4)
+		{
+			GSVector4i v(*(u32*)&src[srcpitch * 0], *(u32*)&src[srcpitch * 2], *(u32*)&src[srcpitch * 1], *(u32*)&src[srcpitch * 3]);
+
+			v4 = (v << 4).upl8(v);
+			v5 = (v << 4).uph8(v);
+
+			v6 = v4.upl16(v5);
+			v7 = v4.uph16(v5);
+
+			v4 = v6.upl8(v6);
+			v5 = v6.uph8(v6);
+			v6 = v7.upl8(v7);
+			v7 = v7.uph8(v7);
+
+			v0 = v4.upl16(v4);
+			v1 = v4.uph16(v4);
+			v2 = v5.upl16(v5);
+			v3 = v5.uph16(v5);
+
+			((GSVector4i*)dst)[i * 8 + 0] = ((GSVector4i*)dst)[i * 8 + 0].blend(v0, mask);
+			((GSVector4i*)dst)[i * 8 + 1] = ((GSVector4i*)dst)[i * 8 + 1].blend(v1, mask);
+			((GSVector4i*)dst)[i * 8 + 2] = ((GSVector4i*)dst)[i * 8 + 2].blend(v2, mask);
+			((GSVector4i*)dst)[i * 8 + 3] = ((GSVector4i*)dst)[i * 8 + 3].blend(v3, mask);
+
+			v0 = v6.upl16(v6);
+			v1 = v6.uph16(v6);
+			v2 = v7.upl16(v7);
+			v3 = v7.uph16(v7);
+
+			((GSVector4i*)dst)[i * 8 + 4] = ((GSVector4i*)dst)[i * 8 + 4].blend(v0, mask);
+			((GSVector4i*)dst)[i * 8 + 5] = ((GSVector4i*)dst)[i * 8 + 5].blend(v1, mask);
+			((GSVector4i*)dst)[i * 8 + 6] = ((GSVector4i*)dst)[i * 8 + 6].blend(v2, mask);
+			((GSVector4i*)dst)[i * 8 + 7] = ((GSVector4i*)dst)[i * 8 + 7].blend(v3, mask);
+		}
+
 #endif
 	}
 
@@ -1654,6 +1882,39 @@ public:
 			d1[1] = Expand16to32<AEM>(v1.uph16(v1), TA0, TA1);
 		}
 
+#elif 0 // not faster
+
+		const GSVector4i* s = (const GSVector4i*)src;
+
+		GSVector4i TA0(TEXA.TA0 << 24);
+		GSVector4i TA1(TEXA.TA1 << 24);
+
+		for(int i = 0; i < 4; i++, dst += dstpitch * 2)
+		{
+			GSVector4i v0 = s[i * 4 + 0];
+			GSVector4i v1 = s[i * 4 + 1];
+			GSVector4i v2 = s[i * 4 + 2];
+			GSVector4i v3 = s[i * 4 + 3];
+
+			GSVector4i::sw16(v0, v1, v2, v3);
+			GSVector4i::sw32(v0, v1, v2, v3);
+			GSVector4i::sw16(v0, v2, v1, v3);
+
+			GSVector4i* d0 = (GSVector4i*)&dst[dstpitch * 0];
+
+			d0[0] = Expand16to32<AEM>(v0.upl16(v0), TA0, TA1);
+			d0[1] = Expand16to32<AEM>(v0.uph16(v0), TA0, TA1);
+			d0[2] = Expand16to32<AEM>(v1.upl16(v1), TA0, TA1);
+			d0[3] = Expand16to32<AEM>(v1.uph16(v1), TA0, TA1);
+			
+			GSVector4i* d1 = (GSVector4i*)&dst[dstpitch * 1];
+
+			d1[0] = Expand16to32<AEM>(v2.upl16(v2), TA0, TA1);
+			d1[1] = Expand16to32<AEM>(v2.uph16(v2), TA0, TA1);
+			d1[2] = Expand16to32<AEM>(v3.upl16(v3), TA0, TA1);
+			d1[3] = Expand16to32<AEM>(v3.uph16(v3), TA0, TA1);
+		}
+
 #else
 
 		alignas(32) u16 block[16 * 8];
@@ -1669,6 +1930,8 @@ public:
 	{
 		//printf("ReadAndExpandBlock8_32\n");
 
+#if _M_SSE >= 0x401
+
 		const GSVector4i* s = (const GSVector4i*)src;
 
 		GSVector4i v0, v1, v2, v3;
@@ -1710,6 +1973,17 @@ public:
 			v2.gather32_8<>(pal, (GSVector4i*)dst);
 			dst += dstpitch;
 		}
+
+#else
+
+		alignas(32) u8 block[16 * 16];
+
+		ReadBlock8(src, (u8*)block, sizeof(block) / 16);
+
+		ExpandBlock8_32(block, dst, dstpitch, pal);
+
+#endif
+
 	}
 
 	// TODO: ReadAndExpandBlock8_16
@@ -1718,6 +1992,8 @@ public:
 	{
 		//printf("ReadAndExpandBlock4_32\n");
 
+#if _M_SSE >= 0x401
+
 		const GSVector4i* s = (const GSVector4i*)src;
 
 		GSVector4i v0, v1, v2, v3;
@@ -1775,6 +2051,17 @@ public:
 			v3.gather64_8<>(pal, (GSVector4i*)dst);
 			dst += dstpitch;
 		}
+
+#else
+
+		alignas(32) u8 block[(32 / 2) * 16];
+
+		ReadBlock4(src, (u8*)block, sizeof(block) / 16);
+
+		ExpandBlock4_32(block, dst, dstpitch, pal);
+
+#endif
+
 	}
 
 	// TODO: ReadAndExpandBlock4_16
@@ -1783,6 +2070,8 @@ public:
 	{
 		//printf("ReadAndExpandBlock8H_32\n");
 
+#if _M_SSE >= 0x401
+
 		const GSVector4i* s = (const GSVector4i*)src;
 
 		GSVector4i v0, v1, v2, v3;
@@ -1806,6 +2095,17 @@ public:
 
 			dst += dstpitch;
 		}
+
+#else
+
+		alignas(32) u32 block[8 * 8];
+
+		ReadBlock32(src, (u8*)block, sizeof(block) / 8);
+
+		ExpandBlock8H_32(block, dst, dstpitch, pal);
+
+#endif
+
 	}
 
 	// TODO: ReadAndExpandBlock8H_16
@@ -1813,6 +2113,9 @@ public:
 	__forceinline static void ReadAndExpandBlock4HL_32(const u8* RESTRICT src, u8* RESTRICT dst, int dstpitch, const u32* RESTRICT pal)
 	{
 		//printf("ReadAndExpandBlock4HL_32\n");
+
+#if _M_SSE >= 0x401
+
 		const GSVector4i* s = (const GSVector4i*)src;
 
 		GSVector4i v0, v1, v2, v3;
@@ -1836,6 +2139,17 @@ public:
 
 			dst += dstpitch;
 		}
+
+#else
+
+		alignas(32) u32 block[8 * 8];
+
+		ReadBlock32(src, (u8*)block, sizeof(block) / 8);
+
+		ExpandBlock4HL_32(block, dst, dstpitch, pal);
+
+#endif
+
 	}
 
 	// TODO: ReadAndExpandBlock4HL_16
@@ -1844,6 +2158,8 @@ public:
 	{
 		//printf("ReadAndExpandBlock4HH_32\n");
 
+#if _M_SSE >= 0x401
+
 		const GSVector4i* s = (const GSVector4i*)src;
 
 		GSVector4i v0, v1, v2, v3;
@@ -1867,6 +2183,17 @@ public:
 
 			dst += dstpitch;
 		}
+
+#else
+
+		alignas(32) u32 block[8 * 8];
+
+		ReadBlock32(src, (u8*)block, sizeof(block) / 8);
+
+		ExpandBlock4HH_32(block, dst, dstpitch, pal);
+
+#endif
+
 	}
 
 	// TODO: ReadAndExpandBlock4HH_16
diff --git a/pcsx2/GS/GSCapture.cpp b/pcsx2/GS/GSCapture.cpp
index 153e247..8dfbd10 100644
--- a/pcsx2/GS/GSCapture.cpp
+++ b/pcsx2/GS/GSCapture.cpp
@@ -329,6 +329,7 @@ public:
 			{
 				if (rgba)
 				{
+#if _M_SSE >= 0x301
 					GSVector4i* s = (GSVector4i*)src;
 					GSVector4i* d = (GSVector4i*)dst;
 
@@ -338,6 +339,15 @@ public:
 					{
 						d[i] = s[i].shuffle8(mask);
 					}
+#else
+					GSVector4i* s = (GSVector4i*)src;
+					GSVector4i* d = (GSVector4i*)dst;
+
+					for (int i = 0, w4 = w >> 2; i < w4; i++)
+					{
+						d[i] = ((s[i] & 0x00ff0000) >> 16) | ((s[i] & 0x000000ff) << 16) | (s[i] & 0x0000ff00);
+					}
+#endif
 				}
 				else
 				{
diff --git a/pcsx2/GS/GSState.cpp b/pcsx2/GS/GSState.cpp
index c52d967..0dfc69e 100644
--- a/pcsx2/GS/GSState.cpp
+++ b/pcsx2/GS/GSState.cpp
@@ -550,11 +550,18 @@ void GSState::GIFPackedRegHandlerNull(const GIFPackedReg* RESTRICT r)
 
 void GSState::GIFPackedRegHandlerRGBA(const GIFPackedReg* RESTRICT r)
 {
+#if _M_SSE >= 0x301
 	const GSVector4i mask = GSVector4i::load(0x0c080400);
 	const GSVector4i v = GSVector4i::load<false>(r).shuffle8(mask);
 
 	m_v.RGBAQ.U32[0] = (u32)GSVector4i::store(v);
 
+#else
+	GSVector4i v = GSVector4i::load<false>(r) & GSVector4i::x000000ff();
+
+	m_v.RGBAQ.U32[0] = v.rgba32();
+#endif
+
 	m_v.RGBAQ.Q = m_q;
 }
 
@@ -2435,7 +2442,11 @@ __forceinline void GSState::VertexKick(u32 skip)
 
 	const GSVector4i xy = v1.xxxx().u16to32().sub32(m_ofxy);
 
+#if _M_SSE >= 0x401
 	GSVector4i::storel(&m_vertex.xy[xy_tail & 3], xy.blend16<0xf0>(xy.sra32(4)).ps32());
+#else
+	GSVector4i::storel(&m_vertex.xy[xy_tail & 3], xy.upl64(xy.sra32(4).zwzw()).ps32());
+#endif
 
 	m_vertex.tail = ++tail;
 	m_vertex.xy_tail = ++xy_tail;
diff --git a/pcsx2/GS/GSUtil.cpp b/pcsx2/GS/GSUtil.cpp
index 502b820..5573dfd 100644
--- a/pcsx2/GS/GSUtil.cpp
+++ b/pcsx2/GS/GSUtil.cpp
@@ -159,7 +159,13 @@ bool GSUtil::CheckSSE()
 	};
 
 	ISA checks[] = {
+		{Xbyak::util::Cpu::tSSE2, "SSE2"},
+#if _M_SSE >= 0x301
+		{Xbyak::util::Cpu::tSSSE3, "SSSE3"},
+#endif
+#if _M_SSE >= 0x401
 		{Xbyak::util::Cpu::tSSE41, "SSE41"},
+#endif
 #if _M_SSE >= 0x500
 		{Xbyak::util::Cpu::tAVX, "AVX1"},
 #endif
diff --git a/pcsx2/GS/GSVector4.h b/pcsx2/GS/GSVector4.h
index 3304b30..bf3b5f6 100644
--- a/pcsx2/GS/GSVector4.h
+++ b/pcsx2/GS/GSVector4.h
@@ -256,7 +256,30 @@ public:
 	template <int mode>
 	__forceinline GSVector4 round() const
 	{
+#if _M_SSE >= 0x401
 		return GSVector4(_mm_round_ps(m, mode));
+#else
+		GSVector4 a = *this;
+
+		GSVector4 b = (a & cast(GSVector4i::x80000000())) | m_x4b000000;
+
+		b = a + b - b;
+
+		if((mode & 7) == (Round_NegInf & 7))
+		{
+			return b - ((a < b) & m_one);
+		}
+
+		if((mode & 7) == (Round_PosInf & 7))
+		{
+			return b + ((a > b) & m_one);
+		}
+
+		ASSERT((mode & 7) == (Round_NearestInt & 7)); // other modes aren't implemented
+
+		return b;
+
+#endif
 	}
 
 	__forceinline GSVector4 floor() const
@@ -384,29 +407,47 @@ public:
 
 	__forceinline GSVector4 hadd() const
 	{
+#if _M_SSE >= 0x300
 		return GSVector4(_mm_hadd_ps(m, m));
+#else
+		return xzxz() + ywyw();
+#endif
 	}
 
 	__forceinline GSVector4 hadd(const GSVector4& v) const
 	{
+#if _M_SSE >= 0x300
 		return GSVector4(_mm_hadd_ps(m, v.m));
+#else
+		return xzxz(v) + ywyw(v);
+#endif
 	}
 
 	__forceinline GSVector4 hsub() const
 	{
+#if _M_SSE >= 0x300
 		return GSVector4(_mm_hsub_ps(m, m));
+#else
+		return xzxz() - ywyw();
+#endif
 	}
 
 	__forceinline GSVector4 hsub(const GSVector4& v) const
 	{
+#if _M_SSE >= 0x300
 		return GSVector4(_mm_hsub_ps(m, v.m));
+#else
+		return xzxz(v) - ywyw(v);
+#endif
 	}
 
+#if _M_SSE >= 0x401
 	template <int i>
 	__forceinline GSVector4 dp(const GSVector4& v) const
 	{
 		return GSVector4(_mm_dp_ps(m, v.m, i));
 	}
+#endif
 
 	__forceinline GSVector4 sat(const GSVector4& a, const GSVector4& b) const
 	{
@@ -438,15 +479,21 @@ public:
 		return GSVector4(_mm_max_ps(m, a));
 	}
 
+#if _M_SSE >= 0x401
 	template <int mask>
 	__forceinline GSVector4 blend32(const GSVector4& a) const
 	{
 		return GSVector4(_mm_blend_ps(m, a, mask));
 	}
+#endif
 
 	__forceinline GSVector4 blend32(const GSVector4& a, const GSVector4& mask) const
 	{
+#if _M_SSE >= 0x401
 		return GSVector4(_mm_blendv_ps(m, a, mask));
+#else
+		return GSVector4(_mm_or_ps(_mm_andnot_ps(mask, m), _mm_and_ps(mask, a)));
+#endif
 	}
 
 	__forceinline GSVector4 upl(const GSVector4& a) const
@@ -500,12 +547,16 @@ public:
 
 		return _mm_testz_ps(m, m) != 0;
 
-		#else
+		#elif _M_SSE >= 0x401
 
 		__m128i a = _mm_castps_si128(m);
 
 		return _mm_testz_si128(a, a) != 0;
 
+		#else
+
+		return mask() == 0;
+
 #endif
 	}
 
@@ -594,13 +645,21 @@ GSVector.h:2973:15: error:  shadows template parm 'int i'
 	template <int index>
 	__forceinline int extract32() const
 	{
+#if _M_SSE >= 0x401
 		return _mm_extract_ps(m, index);
+#else
+		return I32[index];
+#endif
 	}
 #else
 	template <int i>
 	__forceinline int extract32() const
 	{
+#if _M_SSE >= 0x401
 		return _mm_extract_ps(m, i);
+#else
+		return I32[i];
+#endif
 	}
 #endif
 
diff --git a/pcsx2/GS/GSVector4i.h b/pcsx2/GS/GSVector4i.h
index 09784d5..36ec449 100644
--- a/pcsx2/GS/GSVector4i.h
+++ b/pcsx2/GS/GSVector4i.h
@@ -224,7 +224,11 @@ public:
 
 	__forceinline GSVector4i runion_ordered(const GSVector4i& a) const
 	{
+#if _M_SSE >= 0x401
 		return min_i32(a).upl64(max_i32(a).srl<8>());
+#else
+		return GSVector4i(std::min(x, a.x), std::min(y, a.y), std::max(z, a.z), std::max(w, a.w));
+#endif
 	}
 
 	__forceinline GSVector4i rintersect(const GSVector4i& a) const
@@ -294,6 +298,7 @@ public:
 		return (u32)store(v);
 	}
 
+#if _M_SSE >= 0x401
 	__forceinline GSVector4i sat_i8(const GSVector4i& a, const GSVector4i& b) const
 	{
 		return max_i8(a).min_i8(b);
@@ -303,6 +308,7 @@ public:
 	{
 		return max_i8(a.xyxy()).min_i8(a.zwzw());
 	}
+#endif
 
 	__forceinline GSVector4i sat_i16(const GSVector4i& a, const GSVector4i& b) const
 	{
@@ -314,6 +320,7 @@ public:
 		return max_i16(a.xyxy()).min_i16(a.zwzw());
 	}
 
+#if _M_SSE >= 0x401
 	__forceinline GSVector4i sat_i32(const GSVector4i& a, const GSVector4i& b) const
 	{
 		return max_i32(a).min_i32(b);
@@ -323,6 +330,31 @@ public:
 	{
 		return max_i32(a.xyxy()).min_i32(a.zwzw());
 	}
+#else
+	__forceinline GSVector4i sat_i32(const GSVector4i& a, const GSVector4i& b) const
+	{
+		GSVector4i v;
+
+		v.x = std::min(std::max(x, a.x), b.x);
+		v.y = std::min(std::max(y, a.y), b.y);
+		v.z = std::min(std::max(z, a.z), b.z);
+		v.w = std::min(std::max(w, a.w), b.w);
+
+		return v;
+	}
+
+	__forceinline GSVector4i sat_i32(const GSVector4i& a) const
+	{
+		GSVector4i v;
+
+		v.x = std::min(std::max(x, a.x), a.z);
+		v.y = std::min(std::max(y, a.y), a.w);
+		v.z = std::min(std::max(z, a.x), a.z);
+		v.w = std::min(std::max(w, a.y), a.w);
+
+		return v;
+	}
+#endif
 
 	__forceinline GSVector4i sat_u8(const GSVector4i& a, const GSVector4i& b) const
 	{
@@ -334,6 +366,7 @@ public:
 		return max_u8(a.xyxy()).min_u8(a.zwzw());
 	}
 
+#if _M_SSE >= 0x401
 	__forceinline GSVector4i sat_u16(const GSVector4i& a, const GSVector4i& b) const
 	{
 		return max_u16(a).min_u16(b);
@@ -343,7 +376,9 @@ public:
 	{
 		return max_u16(a.xyxy()).min_u16(a.zwzw());
 	}
+#endif
 
+#if _M_SSE >= 0x401
 	__forceinline GSVector4i sat_u32(const GSVector4i& a, const GSVector4i& b) const
 	{
 		return max_u32(a).min_u32(b);
@@ -353,7 +388,9 @@ public:
 	{
 		return max_u32(a.xyxy()).min_u32(a.zwzw());
 	}
+#endif
 
+#if _M_SSE >= 0x401
 	__forceinline GSVector4i min_i8(const GSVector4i& a) const
 	{
 		return GSVector4i(_mm_min_epi8(m, a));
@@ -363,6 +400,7 @@ public:
 	{
 		return GSVector4i(_mm_max_epi8(m, a));
 	}
+#endif
 
 	__forceinline GSVector4i min_i16(const GSVector4i& a) const
 	{
@@ -374,6 +412,7 @@ public:
 		return GSVector4i(_mm_max_epi16(m, a));
 	}
 
+#if _M_SSE >= 0x401
 	__forceinline GSVector4i min_i32(const GSVector4i& a) const
 	{
 		return GSVector4i(_mm_min_epi32(m, a));
@@ -383,6 +422,7 @@ public:
 	{
 		return GSVector4i(_mm_max_epi32(m, a));
 	}
+#endif
 
 	__forceinline GSVector4i min_u8(const GSVector4i& a) const
 	{
@@ -394,6 +434,7 @@ public:
 		return GSVector4i(_mm_max_epu8(m, a));
 	}
 
+#if _M_SSE >= 0x401
 	__forceinline GSVector4i min_u16(const GSVector4i& a) const
 	{
 		return GSVector4i(_mm_min_epu16(m, a));
@@ -403,7 +444,9 @@ public:
 	{
 		return GSVector4i(_mm_max_epu16(m, a));
 	}
+#endif
 
+#if _M_SSE >= 0x401
 	__forceinline GSVector4i min_u32(const GSVector4i& a) const
 	{
 		return GSVector4i(_mm_min_epu32(m, a));
@@ -413,6 +456,7 @@ public:
 	{
 		return GSVector4i(_mm_max_epu32(m, a));
 	}
+#endif
 
 	__forceinline static int min_i16(int a, int b)
 	{
@@ -426,15 +470,22 @@ public:
 
 	__forceinline GSVector4i blend8(const GSVector4i& a, const GSVector4i& mask) const
 	{
+#if _M_SSE >= 0x401
 		return GSVector4i(_mm_blendv_epi8(m, a, mask));
+#else
+		return GSVector4i(_mm_or_si128(_mm_andnot_si128(mask, m), _mm_and_si128(mask, a)));
+#endif
 	}
 
+#if _M_SSE >= 0x401
 	template <int mask>
 	__forceinline GSVector4i blend16(const GSVector4i& a) const
 	{
 		return GSVector4i(_mm_blend_epi16(m, a, mask));
 	}
+#endif
 
+#if _M_SSE >= 0x301
 	template <int mask>
 	__forceinline GSVector4i blend32(const GSVector4i& v) const
 	{
@@ -448,6 +499,7 @@ public:
 		return blend16<bit3 | bit2 | bit1 | bit0>(v);
 #endif
 	}
+#endif
 
 	__forceinline GSVector4i blend(const GSVector4i& a, const GSVector4i& mask) const
 	{
@@ -456,13 +508,19 @@ public:
 
 	__forceinline GSVector4i mix16(const GSVector4i& a) const
 	{
+#if _M_SSE >= 0x401
 		return blend16<0xaa>(a);
+#else
+		return blend8(a, GSVector4i::xffff0000());
+#endif
 	}
 
+#if _M_SSE >= 0x301
 	__forceinline GSVector4i shuffle8(const GSVector4i& mask) const
 	{
 		return GSVector4i(_mm_shuffle_epi8(m, mask));
 	}
+#endif
 
 	__forceinline GSVector4i ps16(const GSVector4i& a) const
 	{
@@ -494,6 +552,7 @@ public:
 		return GSVector4i(_mm_packs_epi32(m, m));
 	}
 
+#if _M_SSE >= 0x401
 	__forceinline GSVector4i pu32(const GSVector4i& a) const
 	{
 		return GSVector4i(_mm_packus_epi32(m, a));
@@ -503,6 +562,7 @@ public:
 	{
 		return GSVector4i(_mm_packus_epi32(m, m));
 	}
+#endif
 
 	__forceinline GSVector4i upl8(const GSVector4i& a) const
 	{
@@ -608,6 +668,7 @@ public:
 		return GSVector4i(_mm_unpackhi_epi64(m, _mm_setzero_si128()));
 	}
 
+#if _M_SSE >= 0x401
 	// WARNING!!!
 	//
 	// MSVC (2008, 2010 ctp) believes that there is a "mem, reg" form of the pmovz/sx* instructions,
@@ -673,6 +734,50 @@ public:
 		return GSVector4i(_mm_cvtepu32_epi64(m));
 	}
 
+#else
+
+	__forceinline GSVector4i u8to16() const
+	{
+		return upl8();
+	}
+
+	__forceinline GSVector4i u8to32() const
+	{
+		return upl8().upl16();
+	}
+
+	__forceinline GSVector4i u8to64() const
+	{
+		return upl8().upl16().upl32();
+	}
+
+	__forceinline GSVector4i u16to32() const
+	{
+		return upl16();
+	}
+
+	__forceinline GSVector4i u16to64() const
+	{
+		return upl16().upl32();
+	}
+
+	__forceinline GSVector4i u32to64() const
+	{
+		return upl32();
+	}
+
+	__forceinline GSVector4i i8to16() const
+	{
+		return zero().upl8(*this).sra16(8);
+	}
+
+	__forceinline GSVector4i i16to32() const
+	{
+		return zero().upl16(*this).sra32(16);
+	}
+
+#endif
+
 	template <int i>
 	__forceinline GSVector4i srl() const
 	{
@@ -682,7 +787,16 @@ public:
 	template <int i>
 	__forceinline GSVector4i srl(const GSVector4i& v)
 	{
+#if _M_SSE >= 0x301
 		return GSVector4i(_mm_alignr_epi8(v.m, m, i));
+#else
+		// The `& 0xF` keeps the compiler happy on cases that won't actually be hit
+		if(i == 0) return *this;
+		else if(i < 16) return srl<i & 0xF>() | v.sll<(16 - i) & 0xF>();
+		else if(i == 16) return v;
+		else if(i < 32) return v.srl<(i - 16) & 0xF>();
+		else return zero();
+#endif
 	}
 
 	template <int i>
@@ -887,10 +1001,12 @@ public:
 		return GSVector4i(_mm_mullo_epi16(m, v.m));
 	}
 
+#if _M_SSE >= 0x301
 	__forceinline GSVector4i mul16hrs(const GSVector4i& v) const
 	{
 		return GSVector4i(_mm_mulhrs_epi16(m, v.m));
 	}
+#endif
 
 	GSVector4i madd(const GSVector4i& v) const
 	{
@@ -932,21 +1048,30 @@ public:
 	__forceinline GSVector4i modulate16(const GSVector4i& f) const
 	{
 		// a * f << shift
+#if _M_SSE >= 0x301
 		if (shift == 0)
 		{
 			return mul16hrs(f);
 		}
+#endif
 
 		return sll16(shift + 1).mul16hs(f);
 	}
 
 	__forceinline bool eq(const GSVector4i& v) const
 	{
+#if _M_SSE >= 0x401
 		// pxor, ptest, je
 
 		GSVector4i t = *this ^ v;
 
 		return _mm_testz_si128(t, t) != 0;
+#else
+
+		// pcmpeqd, pmovmskb, cmp, je
+
+		return eq32(v).alltrue();
+#endif
 	}
 
 	__forceinline GSVector4i eq8(const GSVector4i& v) const
@@ -1026,19 +1151,29 @@ public:
 
 	__forceinline bool allfalse() const
 	{
+#if _M_SSE >= 0x401
 		return _mm_testz_si128(m, m) != 0;
+#else
+		return mask() == 0;
+#endif
 	}
 
+#if _M_SSE >= 0x401
 	template <int i>
 	__forceinline GSVector4i insert8(int a) const
 	{
 		return GSVector4i(_mm_insert_epi8(m, a, i));
 	}
+#endif
 
 	template <int i>
 	__forceinline int extract8() const
 	{
+#if _M_SSE >= 0x401
 		return _mm_extract_epi8(m, i);
+#else
+		return (int)U8[i];
+#endif
 	}
 
 	template <int i>
@@ -1053,11 +1188,13 @@ public:
 		return _mm_extract_epi16(m, i);
 	}
 
+#if _M_SSE >= 0x401
 	template <int i>
 	__forceinline GSVector4i insert32(int a) const
 	{
 		return GSVector4i(_mm_insert_epi32(m, a, i));
 	}
+#endif
 
 	template <int i>
 	__forceinline int extract32() const
@@ -1065,16 +1202,22 @@ public:
 		if (i == 0)
 			return GSVector4i::store(*this);
 
+#if _M_SSE >= 0x401
 		return _mm_extract_epi32(m, i);
+#else
+		return I32[i];
+#endif
 	}
 
 #ifdef _M_AMD64
 
+#if _M_SSE >= 0x401
 	template <int i>
 	__forceinline GSVector4i insert64(s64 a) const
 	{
 		return GSVector4i(_mm_insert_epi64(m, a, i));
 	}
+#endif
 
 	template <int i>
 	__forceinline s64 extract64() const
@@ -1082,11 +1225,16 @@ public:
 		if (i == 0)
 			return GSVector4i::storeq(*this);
 
+#if _M_SSE >= 0x401
 		return _mm_extract_epi64(m, i);
+#else
+		return I64[i];
+#endif
 	}
 
 #endif
 
+#if _M_SSE >= 0x401
 	template <int src, class T>
 	__forceinline GSVector4i gather8_4(const T* ptr) const
 	{
@@ -1183,6 +1331,7 @@ public:
 
 		return v;
 	}
+#endif
 
 	template <int src, class T>
 	__forceinline GSVector4i gather16_8(const T* ptr) const
@@ -1248,6 +1397,7 @@ public:
 		return v;
 	}
 
+#if _M_SSE >= 0x401
 	template <int src, class T>
 	__forceinline GSVector4i gather32_4(const T* ptr) const
 	{
@@ -1311,8 +1461,55 @@ public:
 
 		return v;
 	}
+#else
 
-#if defined(_M_AMD64)
+	template<int src, class T> __forceinline GSVector4i gather32_4(const T* ptr) const
+	{
+		return GSVector4i(
+			(int)ptr[extract8<src + 0>() & 0xf],
+			(int)ptr[extract8<src + 0>() >> 4],
+			(int)ptr[extract8<src + 1>() & 0xf],
+			(int)ptr[extract8<src + 1>() >> 4]);
+	}
+
+	template<int src, class T> __forceinline GSVector4i gather32_8(const T* ptr) const
+	{
+		return GSVector4i(
+			(int)ptr[extract8<src + 0>()],
+			(int)ptr[extract8<src + 1>()],
+			(int)ptr[extract8<src + 2>()],
+			(int)ptr[extract8<src + 3>()]);
+	}
+
+	template<int src, class T> __forceinline GSVector4i gather32_16(const T* ptr) const
+	{
+		return GSVector4i(
+			(int)ptr[extract16<src + 0>()],
+			(int)ptr[extract16<src + 1>()],
+			(int)ptr[extract16<src + 2>()],
+			(int)ptr[extract16<src + 3>()]);
+	}
+
+	template<class T> __forceinline GSVector4i gather32_32(const T* ptr) const
+	{
+		return GSVector4i(
+			(int)ptr[extract32<0>()],
+			(int)ptr[extract32<1>()],
+			(int)ptr[extract32<2>()],
+			(int)ptr[extract32<3>()]);
+	}
+
+	template<class T1, class T2> __forceinline GSVector4i gather32_32(const T1* ptr1, const T2* ptr2) const
+	{
+		return GSVector4i(
+			(int)ptr2[ptr1[extract32<0>()]],
+			(int)ptr2[ptr1[extract32<1>()]],
+			(int)ptr2[ptr1[extract32<2>()]],
+			(int)ptr2[ptr1[extract32<3>()]]);
+	}
+#endif
+
+#if defined(_M_AMD64) && _M_SSE >= 0x401
 
 	template <int src, class T>
 	__forceinline GSVector4i gather64_4(const T* ptr) const
@@ -1413,6 +1610,7 @@ public:
 
 #endif
 
+#if _M_SSE >= 0x401
 	template <class T>
 	__forceinline void gather8_4(const T* RESTRICT ptr, GSVector4i* RESTRICT dst) const
 	{
@@ -1424,6 +1622,7 @@ public:
 	{
 		dst[0] = gather8_8<>(ptr);
 	}
+#endif
 
 	template <class T>
 	__forceinline void gather16_4(const T* RESTRICT ptr, GSVector4i* RESTRICT dst) const
@@ -1544,7 +1743,11 @@ public:
 
 	__forceinline static GSVector4i loadnt(const void* p)
 	{
+#if _M_SSE >= 0x401
 		return GSVector4i(_mm_stream_load_si128((__m128i*)p));
+#else
+		return GSVector4i(_mm_load_si128((__m128i*)p));
+#endif
 	}
 
 	__forceinline static GSVector4i loadl(const void* p)
diff --git a/pcsx2/GS/Renderers/Common/GSVertexTrace.cpp b/pcsx2/GS/Renderers/Common/GSVertexTrace.cpp
index 5c0dd56..b3f4b52 100644
--- a/pcsx2/GS/Renderers/Common/GSVertexTrace.cpp
+++ b/pcsx2/GS/Renderers/Common/GSVertexTrace.cpp
@@ -29,6 +29,7 @@ GSVertexTrace::GSVertexTrace(const GSState* state)
 	#define InitUpdate3(P, IIP, TME, FST, COLOR) \
 		m_fmm[COLOR][FST][TME][IIP][P] = &GSVertexTrace::FindMinMax<P, IIP, TME, FST, COLOR>;
 
+
 	#define InitUpdate2(P, IIP, TME) \
 		InitUpdate3(P, IIP, TME, 0, 0) \
 		InitUpdate3(P, IIP, TME, 0, 1) \
@@ -174,8 +175,13 @@ void GSVertexTrace::FindMinMax(const void* vertex, const u32* index, int count)
 	GSVector4i cmin = GSVector4i::xffffffff();
 	GSVector4i cmax = GSVector4i::zero();
 
+#if _M_SSE >= 0x401
 	GSVector4i pmin = GSVector4i::xffffffff();
 	GSVector4i pmax = GSVector4i::zero();
+#else
+	GSVector4 pmin = s_minmax.xxxx();
+	GSVector4 pmax = s_minmax.yyyy();
+#endif
 
 	const GSVertex* RESTRICT v = (GSVertex*)vertex;
 
@@ -247,11 +253,19 @@ void GSVertexTrace::FindMinMax(const void* vertex, const u32* index, int count)
 		GSVector4i xy1 = xyzf1.upl16();
 		GSVector4i z1 = xyzf1.yyyy();
 
+#if _M_SSE >= 0x401
 		GSVector4i p0 = xy0.blend16<0xf0>(z0.uph32(primclass == GS_SPRITE_CLASS ? xyzf1 : xyzf0));
 		GSVector4i p1 = xy1.blend16<0xf0>(z1.uph32(xyzf1));
 
 		pmin = pmin.min_u32(p0.min_u32(p1));
 		pmax = pmax.max_u32(p0.max_u32(p1));
+#else
+		GSVector4 p0 = GSVector4(xy0.upl64(z0.srl32(1).upl32(xyzf1.wwww())));
+		GSVector4 p1 = GSVector4(xy1.upl64(z1.srl32(1).upl32(xyzf1.wwww())));
+
+		pmin = pmin.min(p0.min(p1));
+		pmax = pmax.max(p0.max(p1));
+#endif
 	};
 
 	if (n == 2)
@@ -297,6 +311,11 @@ void GSVertexTrace::FindMinMax(const void* vertex, const u32* index, int count)
 		pxAssertRel(0, "Bad n value");
 	}
 
+#if _M_SSE >= 0x401
+	pmin = pmin.blend16<0x30>(pmin.srl32(1));
+	pmax = pmax.blend16<0x30>(pmax.srl32(1));
+#endif
+
 	GSVector4 o(context->XYOFFSET);
 	GSVector4 s(1.0f / 16, 1.0f / 16, 2.0f, 1.0f);
 
diff --git a/pcsx2/GS/Renderers/HW/GSRendererNew.cpp b/pcsx2/GS/Renderers/HW/GSRendererNew.cpp
index 5f1bbf0..e442c70 100644
--- a/pcsx2/GS/Renderers/HW/GSRendererNew.cpp
+++ b/pcsx2/GS/Renderers/HW/GSRendererNew.cpp
@@ -1381,7 +1381,11 @@ void GSRendererNew::DrawPrims(GSTexture* rt, GSTexture* ds, GSTextureCache::Sour
 
 		const GSVector4 fc = GSVector4::rgba32(m_env.FOGCOL.U32[0]);
 		// Blend AREF to avoid to load a random value for alpha (dirty cache)
+#if _M_SSE >= 0x401
 		m_conf.cb_ps.FogColor_AREF = fc.blend32<8>(m_conf.cb_ps.FogColor_AREF);
+#else
+		m_conf.cb_ps.FogColor_AREF = fc;
+#endif
 	}
 
 	// Warning must be done after EmulateZbuffer
diff --git a/pcsx2/GS/Renderers/SW/GSDrawScanlineCodeGenerator.all.cpp b/pcsx2/GS/Renderers/SW/GSDrawScanlineCodeGenerator.all.cpp
index 65b3808..953c252 100644
--- a/pcsx2/GS/Renderers/SW/GSDrawScanlineCodeGenerator.all.cpp
+++ b/pcsx2/GS/Renderers/SW/GSDrawScanlineCodeGenerator.all.cpp
@@ -231,7 +231,18 @@ void GSDrawScanlineCodeGenerator2::lerp16_4(const XYm& a, const XYm& b, const XY
 
 void GSDrawScanlineCodeGenerator2::mix16(const XYm& a, const XYm& b, const XYm& temp)
 {
-	pblendw(a, b, 0xaa);
+	if (hasAVX)
+	{
+		pblendw(a, b, 0xaa);
+	}
+	else
+	{
+		pcmpeqd(temp, temp);
+		psrld(temp, 16);
+		pand(a, temp);
+		pandn(temp, b);
+		por(a, temp);
+	}
 }
 
 void GSDrawScanlineCodeGenerator2::clamp16(const XYm& a, const XYm& temp)
@@ -281,7 +292,10 @@ void GSDrawScanlineCodeGenerator2::blendr(const XYm& b, const XYm& a, const XYm&
 
 void GSDrawScanlineCodeGenerator2::blend8(const XYm& a, const XYm& b)
 {
-	pblendvb(a, b /*, xym0 */);
+	if (hasAVX)
+		pblendvb(a, b /*, xym0 */);
+	else
+		blend(a, b, xmm0);
 }
 
 void GSDrawScanlineCodeGenerator2::blend8r(const XYm& b, const XYm& a)
@@ -290,11 +304,15 @@ void GSDrawScanlineCodeGenerator2::blend8r(const XYm& b, const XYm& a)
 	{
 		vpblendvb(b, a, b, xym0);
 	}
-	else
+	else if (x86caps.hasStreamingSIMD4Extensions)
 	{
 		pblendvb(a, b);
 		movdqa(b, a);
 	}
+	else
+	{
+		blendr(b, a, xmm0);
+	}
 }
 
 void GSDrawScanlineCodeGenerator2::split16_2x8(const XYm& l, const XYm& h, const XYm& src)
@@ -1235,9 +1253,20 @@ void GSDrawScanlineCodeGenerator2::TestZ(const XYm& temp1, const XYm& temp2)
 		if (m_sel.zclamp)
 		{
 			const u8 amt = (u8)((m_sel.zpsm & 0x3) * 8);
+#if _M_SSE >= 0x401
 			pcmpeqd(temp1, temp1);
 			psrld(temp1, amt);
 			pminsd(xym0, temp1);
+#else
+			pcmpeqd(temp1, temp1);
+			psrld(temp1, amt);
+			pcmpgtd(temp1, xmm0);
+			pand(xmm0, temp1);
+			pcmpeqd(temp2, temp2);
+			pxor(temp1, temp2);
+			psrld(temp1, amt);
+			por(xmm0, temp1);
+#endif
 		}
 
 		if (m_sel.zwrite)
@@ -1714,7 +1743,10 @@ void GSDrawScanlineCodeGenerator2::Wrap(const XYm& uv0, const XYm& uv1)
 			pmaxsw(uv, min);
 			pminsw(uv, max);
 			// clamp.blend8(repeat, m_local.gd->t.mask);
-			pblendvb(uv, tmp /*, xym0==mask */);
+			if (x86caps.hasStreamingSIMD4Extensions)
+				pblendvb(uv, tmp /*, xym0==mask */);
+			else
+				blendr(uv, tmp, xmm6);
 		}
 	}
 }
@@ -2234,18 +2266,30 @@ void GSDrawScanlineCodeGenerator2::WrapLOD(const XYm& uv0, const XYm& uv1)
 	{
 		broadcasti128(mask, _rip_global(t.mask));
 
-		for (const XYm& uv : {uv0, uv1})
-		{
-			// GSVector4i repeat = (t & m_local.gd->t.min) | m_local.gd->t.max;
-			THREEARG(pand, tmp, uv, min);
-			if (region)
-				por(tmp, max);
-			// GSVector4i clamp = t.sat_i16(m_local.gd->t.min, m_local.gd->t.max);
-			pmaxsw(uv, min);
-			pminsw(uv, max);
-			// clamp.blend8(repeat, m_local.gd->t.mask);*
-			pblendvb(uv, tmp /*, xym0==mask */);
-		}
+		// GSVector4i repeat = (t & m_local.gd->t.min) | m_local.gd->t.max;
+		THREEARG(pand, tmp, uv0, min);
+		if (region)
+			por(tmp, max);
+		// GSVector4i clamp = t.sat_i16(m_local.gd->t.min, m_local.gd->t.max);
+		pmaxsw(uv0, min);
+		pminsw(uv0, max);
+		// clamp.blend8(repeat, m_local.gd->t.mask);*
+		if (x86caps.hasStreamingSIMD4Extensions)
+			pblendvb(uv0, tmp /*, xym0==mask */);
+		else
+			blendr(uv0, tmp, xmm0);
+
+		THREEARG(pand, tmp, uv1, min);
+		if (region)
+			por(tmp, max);
+		// GSVector4i clamp = t.sat_i16(m_local.gd->t.min, m_local.gd->t.max);
+		pmaxsw(uv1, min);
+		pminsw(uv1, max);
+		// clamp.blend8(repeat, m_local.gd->t.mask);*
+		if (x86caps.hasStreamingSIMD4Extensions)
+			pblendvb(uv1, tmp /*, xym0==mask */);
+		else
+			blendr(uv1, tmp, xmm4);
 	}
 }
 
@@ -2763,9 +2807,20 @@ void GSDrawScanlineCodeGenerator2::WriteZBuf()
 	if (m_sel.zclamp)
 	{
 		const u8 amt = (u8)((m_sel.zpsm & 0x3) * 8);
+#if _M_SSE >= 0x401
 		pcmpeqd(xym7, xym7);
 		psrld(xym7, amt);
 		pminsd(xym1, xym7);
+#else
+		static GSVector4i all_1s = GSVector4i(0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff);
+		pcmpeqd(xmm7, xmm7);
+		psrld(xmm7, amt);
+		pcmpgtd(xmm7, xmm1);
+		pand(xmm1, xmm7);
+		pxor(xmm7, ptr[&all_1s]);
+		psrld(xmm7, amt);
+		por(xmm1, xmm7);
+#endif
 	}
 
 	bool fast = m_sel.ztest ? m_sel.zpsm < 2 : m_sel.zpsm == 0 && m_sel.notest;
@@ -3030,6 +3085,15 @@ void GSDrawScanlineCodeGenerator2::AlphaBlend()
 
 	if (m_sel.pabe)
 	{
+		if (x86caps.hasStreamingSIMD4Extensions)
+		{
+			// doh, previous blend8r overwrote xmm0 (sse41 uses pblendvb)
+			movdqa(xmm0, xmm4);
+			pslld(xmm0, 8);
+			psrad(xmm0, 31);
+
+		}
+
 		psrld(xym0, 16); // zero out high words to select the source alpha in blend (so it also does mix16)
 
 		// ga = c[1].blend8(ga, mask).mix16(c[1]);
diff --git a/pcsx2/GS/Renderers/SW/GSRendererSW.cpp b/pcsx2/GS/Renderers/SW/GSRendererSW.cpp
index 215c185..4e12692 100644
--- a/pcsx2/GS/Renderers/SW/GSRendererSW.cpp
+++ b/pcsx2/GS/Renderers/SW/GSRendererSW.cpp
@@ -260,16 +260,27 @@ void GSRendererSW::ConvertVertexBuffer(GSVertexSW* RESTRICT dst, const GSVertex*
 
 	GSVector4i off = (GSVector4i)m_context->XYOFFSET;
 	GSVector4 tsize = GSVector4(0x10000 << m_context->TEX0.TW, 0x10000 << m_context->TEX0.TH, 1, 0);
+#if _M_SSE >= 0x401
 	GSVector4i z_max = GSVector4i::xffffffff().srl32(GSLocalMemory::m_psm[m_context->ZBUF.PSM].fmt * 8);
+#else
+	uint32_t z_max = 0xffffffff >> (GSLocalMemory::m_psm[m_context->ZBUF.PSM].fmt * 8);
+#endif
 
 	for (int i = (int)m_vertex.next; i > 0; i--, src++, dst++)
 	{
 		GSVector4 stcq = GSVector4::load<true>(&src->m[0]); // s t rgba q
 
+#if _M_SSE >= 0x401
 		GSVector4i xyzuvf(src->m[1]);
 
 		GSVector4i xy = xyzuvf.upl16() - off;
 		GSVector4i zf = xyzuvf.ywww().min_u32(GSVector4i::xffffff00());
+#else
+		u32 z = src->XYZ.Z;
+
+		GSVector4i xy = GSVector4i::load((int)src->XYZ.U32[0]).upl16() - off;
+		GSVector4i zf = GSVector4i((int)std::min<u32>(z, 0xffffff00), src->FOG); // NOTE: larger values of z may roll over to 0 when converting back to uint32 later
+#endif
 
 		dst->p = GSVector4(xy).xyxy(GSVector4(zf) + (GSVector4::m_x4f800000 & GSVector4::cast(zf.sra32(31)))) * m_pos_scale;
 		dst->c = GSVector4(GSVector4i::cast(stcq).zzzz().u8to32() << 7);
@@ -280,7 +291,11 @@ void GSRendererSW::ConvertVertexBuffer(GSVertexSW* RESTRICT dst, const GSVertex*
 		{
 			if (fst)
 			{
+#if _M_SSE >= 0x401
 				t = GSVector4(xyzuvf.uph16() << (16 - 4));
+#else
+				t = GSVector4(GSVector4i::load(src->UV).upl16() << (16 - 4));
+#endif
 			}
 			else if (q_div)
 			{
@@ -305,8 +320,13 @@ void GSRendererSW::ConvertVertexBuffer(GSVertexSW* RESTRICT dst, const GSVertex*
 
 		if (primclass == GS_SPRITE_CLASS || m_vt.m_eq.z)
 		{
+#if _M_SSE >= 0x401
 			xyzuvf = xyzuvf.min_u32(z_max);
 			t = t.insert32<1, 3>(GSVector4::cast(xyzuvf));
+#else
+			z = std::min(z, z_max);
+			t = t.insert32<0, 3>(GSVector4::cast(GSVector4i::load(z)));
+#endif
 		}
 
 		dst->t = t;
diff --git a/pcsx2/PCSX2Base.h b/pcsx2/PCSX2Base.h
index 1c3f2e9..b607405 100644
--- a/pcsx2/PCSX2Base.h
+++ b/pcsx2/PCSX2Base.h
@@ -29,9 +29,11 @@
 		#define _M_SSE 0x500
 	#elif defined(__SSE4_1__)
 		#define _M_SSE 0x401
+	#elif defined(__SSE2__)
+		#define _M_SSE 0x200
 	#else
-		#error PCSX2 requires compiling for at least SSE 4.1
+		#error PCSX2 requires compiling for at least SSE 2
 	#endif
-#elif _M_SSE < 0x401
-	#error PCSX2 requires compiling for at least SSE 4.1
+#elif _M_SSE < 0x200
+	#error PCSX2 requires compiling for at least SSE 2
 #endif
diff --git a/pcsx2/Pcsx2Config.cpp b/pcsx2/Pcsx2Config.cpp
index c2e2023..ce8e089 100644
--- a/pcsx2/Pcsx2Config.cpp
+++ b/pcsx2/Pcsx2Config.cpp
@@ -149,7 +149,6 @@ Pcsx2Config::RecompilerOptions::RecompilerOptions()
 	EnableVU1 = true;
 
 	// vu and fpu clamping default to standard overflow.
-	vuOverflow = true;
 	//vuExtraOverflow = false;
 	//vuSignOverflow = false;
 	//vuUnderflow = false;
@@ -740,9 +739,6 @@ void Pcsx2Config::GamefixOptions::Set(GamefixId id, bool enabled)
 		case Fix_VUKickstart:
 			VUKickstartHack = enabled;
 			break;
-		case Fix_VUOverflow:
-			VUOverflowHack = enabled;
-			break;
 			jNO_DEFAULT;
 	}
 }
@@ -782,8 +778,6 @@ bool Pcsx2Config::GamefixOptions::Get(GamefixId id) const
 			return IbitHack;
 		case Fix_VUKickstart:
 			return VUKickstartHack;
-		case Fix_VUOverflow:
-			return VUOverflowHack;
 			jNO_DEFAULT;
 	}
 	return false; // unreachable, but we still need to suppress warnings >_<
@@ -808,7 +802,6 @@ void Pcsx2Config::GamefixOptions::LoadSave(SettingsWrapper& wrap)
 	SettingsWrapBitBool(GoemonTlbHack);
 	SettingsWrapBitBool(IbitHack);
 	SettingsWrapBitBool(VUKickstartHack);
-	SettingsWrapBitBool(VUOverflowHack);
 }
 
 
diff --git a/pcsx2/gui/AppInit.cpp b/pcsx2/gui/AppInit.cpp
index abc1daa..e8e38b4 100644
--- a/pcsx2/gui/AppInit.cpp
+++ b/pcsx2/gui/AppInit.cpp
@@ -54,13 +54,14 @@ void Pcsx2App::DetectCpuAndUserMode()
 	x86caps.CountCores();
 	x86caps.SIMD_EstablishMXCSRmask();
 
-	if (!x86caps.hasStreamingSIMD4Extensions)
+	if (!x86caps.hasStreamingSIMD2Extensions)
 	{
-		// This code will probably never run if the binary was correctly compiled for SSE4
-		// SSE4 is required for any decent speed and is supported by more than decade old x86 CPUs
+		// This code will probably never run if the binary was correctly compiled for SSE2
+		// SSE4 is required by upstream for any decent speed and is supported by more than decade old x86 CPUs
+		// SSE2 works with some games, so this is reverted by chinforpms
 		throw Exception::HardwareDeficiency()
-			.SetDiagMsg(L"Critical Failure: SSE4.1 Extensions not available.")
-			.SetUserMsg(_("SSE4 extensions are not available.  PCSX2 requires a cpu that supports the SSE4.1 instruction set."));
+			.SetDiagMsg(L"Critical Failure: SSE2 Extensions not available.")
+			.SetUserMsg(_("SSE2 extensions are not available.  PCSX2 requires a cpu that supports the SSE2 instruction set."));
 	}
 #endif
 
diff --git a/pcsx2/gui/Panels/GameFixesPanel.cpp b/pcsx2/gui/Panels/GameFixesPanel.cpp
index 19c211e..e52b25f 100644
--- a/pcsx2/gui/Panels/GameFixesPanel.cpp
+++ b/pcsx2/gui/Panels/GameFixesPanel.cpp
@@ -96,10 +96,6 @@ Panels::GameFixesPanel::GameFixesPanel( wxWindow* parent )
 			_("VU Kickstart (Run ahead) to avoid sync problems when reading or writing VU registers"),
 			wxEmptyString
 		},
-		{
-			_("VU Overflow hack to check for possible float overflows (Superman Returns)"),
-			wxEmptyString
-		},
 		{
 			_("VU XGkick Sync - Use accurate timing for VU XGKicks (Slower)"),
 			pxEt(L"Fixes graphical errors on WRC, Erementar Gerad, Tennis Court Smash and others."
diff --git a/pcsx2/x86/iMMI.cpp b/pcsx2/x86/iMMI.cpp
index 15f6816..c23cebe 100644
--- a/pcsx2/x86/iMMI.cpp
+++ b/pcsx2/x86/iMMI.cpp
@@ -225,9 +225,19 @@ void recPMTHL()
 
 	int info = eeRecompileCodeXMM(XMMINFO_READS | XMMINFO_READLO | XMMINFO_READHI | XMMINFO_WRITELO | XMMINFO_WRITEHI);
 
-	xBLEND.PS(xRegisterSSE(EEREC_LO), xRegisterSSE(EEREC_S), 0x5);
-	xSHUF.PS(xRegisterSSE(EEREC_HI), xRegisterSSE(EEREC_S), 0xdd);
-	xSHUF.PS(xRegisterSSE(EEREC_HI), xRegisterSSE(EEREC_HI), 0x72);
+	if ( x86caps.hasStreamingSIMD4Extensions )
+	{
+		xBLEND.PS(xRegisterSSE(EEREC_LO), xRegisterSSE(EEREC_S), 0x5);
+		xSHUF.PS(xRegisterSSE(EEREC_HI), xRegisterSSE(EEREC_S), 0xdd);
+		xSHUF.PS(xRegisterSSE(EEREC_HI), xRegisterSSE(EEREC_HI), 0x72);
+	}
+	else
+	{
+		xSHUF.PS(xRegisterSSE(EEREC_LO), xRegisterSSE(EEREC_S), 0x8d);
+		xSHUF.PS(xRegisterSSE(EEREC_HI), xRegisterSSE(EEREC_S), 0xdd);
+		xSHUF.PS(xRegisterSSE(EEREC_LO), xRegisterSSE(EEREC_LO), 0x72);
+		xSHUF.PS(xRegisterSSE(EEREC_HI), xRegisterSSE(EEREC_HI), 0x72);
+	}
 
 	_clearNeededXMMregs();
 }
@@ -421,16 +431,58 @@ void recPMAXW()
 	EE::Profiler.EmitOp(eeOpcode::PMAXW);
 
 	int info = eeRecompileCodeXMM(XMMINFO_READS | XMMINFO_READT | XMMINFO_WRITED);
-	if (EEREC_S == EEREC_T)
-		xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
-	else if (EEREC_D == EEREC_S)
-		xPMAX.SD(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T));
-	else if (EEREC_D == EEREC_T)
-		xPMAX.SD(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
+	if ( x86caps.hasStreamingSIMD4Extensions )
+	{
+		if (EEREC_S == EEREC_T)
+			xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
+		else if (EEREC_D == EEREC_S)
+			xPMAX.SD(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T));
+		else if (EEREC_D == EEREC_T)
+			xPMAX.SD(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
+		else
+		{
+			xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
+			xPMAX.SD(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T));
+		}
+	}
 	else
 	{
-		xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
-		xPMAX.SD(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T));
+		int t0reg;
+
+		if( EEREC_S == EEREC_T )
+		{
+			xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
+		}
+		else
+		{
+			t0reg = _allocTempXMMreg(XMMT_INT, -1);
+			xMOVDQA(xRegisterSSE(t0reg), xRegisterSSE(EEREC_S));
+			xPCMP.GTD(xRegisterSSE(t0reg), xRegisterSSE(EEREC_T));
+
+			if ( EEREC_D == EEREC_S )
+			{
+				xPAND(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg));
+				xPANDN(xRegisterSSE(t0reg), xRegisterSSE(EEREC_T));
+			}
+			else if ( EEREC_D == EEREC_T )
+			{
+				int t1reg = _allocTempXMMreg(XMMT_INT, -1);
+				xMOVDQA(xRegisterSSE(t1reg), xRegisterSSE(EEREC_T));
+				xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
+				xPAND(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg));
+				xPANDN(xRegisterSSE(t0reg), xRegisterSSE(t1reg));
+				_freeXMMreg(t1reg);
+			}
+			else
+			{
+				xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
+				xPAND(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg));
+				xPANDN(xRegisterSSE(t0reg), xRegisterSSE(EEREC_T));
+			}
+
+			xPOR(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg));
+			_freeXMMreg(t0reg);
+		}
 	}
 	_clearNeededXMMregs();
 }
@@ -1268,7 +1320,20 @@ void recPABSW() //needs clamping
 	xPCMP.EQD(xRegisterSSE(t0reg), xRegisterSSE(t0reg));
 	xPSLL.D(xRegisterSSE(t0reg), 31);
 	xPCMP.EQD(xRegisterSSE(t0reg), xRegisterSSE(EEREC_T)); //0xffffffff if equal to 0x80000000
-	xPABS.D(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T)); //0x80000000 -> 0x80000000
+	if( x86caps.hasSupplementalStreamingSIMD3Extensions )
+	{
+		xPABS.D(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T)); //0x80000000 -> 0x80000000
+	}
+	else
+	{
+		int t1reg = _allocTempXMMreg(XMMT_INT, -1);
+		xMOVDQA(xRegisterSSE(t1reg), xRegisterSSE(EEREC_T));
+		xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T));
+		xPSRA.D(xRegisterSSE(t1reg), 31);
+		xPXOR(xRegisterSSE(EEREC_D), xRegisterSSE(t1reg));
+		xPSUB.D(xRegisterSSE(EEREC_D), xRegisterSSE(t1reg)); //0x80000000 -> 0x80000000
+		_freeXMMreg(t1reg);
+	}
 	xPXOR(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg)); //0x80000000 -> 0x7fffffff
 	_freeXMMreg(t0reg);
 	_clearNeededXMMregs();
@@ -1288,7 +1353,20 @@ void recPABSH()
 	xPCMP.EQW(xRegisterSSE(t0reg), xRegisterSSE(t0reg));
 	xPSLL.W(xRegisterSSE(t0reg), 15);
 	xPCMP.EQW(xRegisterSSE(t0reg), xRegisterSSE(EEREC_T)); //0xffff if equal to 0x8000
-	xPABS.W(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T)); //0x8000 -> 0x8000
+	if( x86caps.hasSupplementalStreamingSIMD3Extensions )
+	{
+		xPABS.W(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T)); //0x8000 -> 0x8000
+	}
+	else
+	{
+		int t1reg = _allocTempXMMreg(XMMT_INT, -1);
+		xMOVDQA(xRegisterSSE(t1reg), xRegisterSSE(EEREC_T));
+		xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T));
+		xPSRA.W(xRegisterSSE(t1reg), 15);
+		xPXOR(xRegisterSSE(EEREC_D), xRegisterSSE(t1reg));
+		xPSUB.W(xRegisterSSE(EEREC_D), xRegisterSSE(t1reg)); //0x8000 -> 0x8000
+		_freeXMMreg(t1reg);
+	}
 	xPXOR(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg)); //0x8000 -> 0x7fff
 	_freeXMMreg(t0reg);
 	_clearNeededXMMregs();
@@ -1309,10 +1387,59 @@ void recPMINW()
 		xPMIN.SD(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T));
 	else if (EEREC_D == EEREC_T)
 		xPMIN.SD(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
-	else
+
+	if ( x86caps.hasStreamingSIMD4Extensions )
 	{
-		xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
-		xPMIN.SD(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T));
+		if ( EEREC_S == EEREC_T )
+			xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
+		else if ( EEREC_D == EEREC_S )
+			xPMIN.SD(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T));
+		else if ( EEREC_D == EEREC_T )
+			xPMIN.SD(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
+		else
+		{
+			xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
+			xPMIN.SD(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T));
+		}
+	}
+ 	else
+ 	{
+		int t0reg;
+
+		if ( EEREC_S == EEREC_T )
+		{
+			xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
+		}
+		else
+		{
+			t0reg = _allocTempXMMreg(XMMT_INT, -1);
+			xMOVDQA(xRegisterSSE(t0reg), xRegisterSSE(EEREC_T));
+			xPCMP.GTD(xRegisterSSE(t0reg), xRegisterSSE(EEREC_S));
+
+			if ( EEREC_D == EEREC_S )
+			{
+				xPAND(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg));
+				xPANDN(xRegisterSSE(t0reg), xRegisterSSE(EEREC_T));
+			}
+			else if ( EEREC_D == EEREC_T )
+			{
+				int t1reg = _allocTempXMMreg(XMMT_INT, -1);
+				xMOVDQA(xRegisterSSE(t1reg), xRegisterSSE(EEREC_T));
+				xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
+				xPAND(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg));
+				xPANDN(xRegisterSSE(t0reg), xRegisterSSE(t1reg));
+				_freeXMMreg(t1reg);
+			}
+			else
+			{
+				xMOVDQA(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_S));
+				xPAND(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg));
+				xPANDN(xRegisterSSE(t0reg), xRegisterSSE(EEREC_T));
+			}
+
+			xPOR(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg));
+			_freeXMMreg(t0reg);
+		}
 	}
 	_clearNeededXMMregs();
 }
@@ -1834,6 +1961,12 @@ void recPMADDW()
 {
 	EE::Profiler.EmitOp(eeOpcode::PMADDW);
 
+	if (!x86caps.hasStreamingSIMD4Extensions) {
+		_deleteEEreg(_Rd_, 0);
+		recCall(Interp::PMADDW);
+		return;
+	}
+
 	int info = eeRecompileCodeXMM((((_Rs_) && (_Rt_)) ? XMMINFO_READS : 0) | (((_Rs_) && (_Rt_)) ? XMMINFO_READT : 0) | (_Rd_ ? XMMINFO_WRITED : 0) | XMMINFO_WRITELO | XMMINFO_WRITEHI | XMMINFO_READLO | XMMINFO_READHI);
 	xSHUF.PS(xRegisterSSE(EEREC_LO), xRegisterSSE(EEREC_HI), 0x88);
 	xPSHUF.D(xRegisterSSE(EEREC_LO), xRegisterSSE(EEREC_LO), 0xd8); // LO = {LO[0], HI[0], LO[2], HI[2]}
@@ -1901,8 +2034,20 @@ void recPSLLVW()
 		}
 		else
 		{
-			xPSHUF.D(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T), 0x88);
-			xPMOVSX.DQ(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_D));
+			if ( x86caps.hasStreamingSIMD4Extensions )
+			{
+				xPSHUF.D(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T), 0x88);
+				xPMOVSX.DQ(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_D));
+			}
+			else
+			{
+				int t0reg = _allocTempXMMreg(XMMT_INT, -1);
+				xPSHUF.D(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T), 0x88);
+				xMOVDQA(xRegisterSSE(t0reg), xRegisterSSE(EEREC_D));
+				xPSRA.D(xRegisterSSE(t0reg), 31);
+				xPUNPCK.LDQ(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg));
+				_freeXMMreg(t0reg);
+			}
 		}
 	}
 	else if (_Rt_ == 0)
@@ -1932,8 +2077,18 @@ void recPSLLVW()
 		xPSLL.D(xRegisterSSE(t1reg), xRegisterSSE(t0reg));
 
 		// merge & sign extend
-		xPUNPCK.LDQ(xRegisterSSE(EEREC_D), xRegisterSSE(t1reg));
-		xPMOVSX.DQ(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_D));
+		if ( x86caps.hasStreamingSIMD4Extensions )
+		{
+			xPUNPCK.LDQ(xRegisterSSE(EEREC_D), xRegisterSSE(t1reg));
+			xPMOVSX.DQ(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_D));
+		}
+		else
+		{
+			xPUNPCK.LDQ(xRegisterSSE(EEREC_D), xRegisterSSE(t1reg));
+			xMOVDQA(xRegisterSSE(t0reg), xRegisterSSE(EEREC_D));
+			xPSRA.D(xRegisterSSE(t0reg), 31); // get the signs
+			xPUNPCK.LDQ(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg));
+		}
 
 		_freeXMMreg(t0reg);
 		_freeXMMreg(t1reg);
@@ -1958,8 +2113,20 @@ void recPSRLVW()
 		}
 		else
 		{
-			xPSHUF.D(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T), 0x88);
-			xPMOVSX.DQ(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_D));
+			if ( x86caps.hasStreamingSIMD4Extensions )
+			{
+				xPSHUF.D(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T), 0x88);
+				xPMOVSX.DQ(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_D));
+			}
+			else
+			{
+				int t0reg = _allocTempXMMreg(XMMT_INT, -1);
+				xPSHUF.D(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T), 0x88);
+				xMOVDQA(xRegisterSSE(t0reg), xRegisterSSE(EEREC_D));
+				xPSRA.D(xRegisterSSE(t0reg), 31);
+				xPUNPCK.LDQ(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg));
+				_freeXMMreg(t0reg);
+			}
 		}
 	}
 	else if (_Rt_ == 0)
@@ -1989,8 +2156,18 @@ void recPSRLVW()
 		xPSRL.D(xRegisterSSE(t1reg), xRegisterSSE(t0reg));
 
 		// merge & sign extend
-		xPUNPCK.LDQ(xRegisterSSE(EEREC_D), xRegisterSSE(t1reg));
-		xPMOVSX.DQ(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_D));
+		if ( x86caps.hasStreamingSIMD4Extensions )
+		{
+			xPUNPCK.LDQ(xRegisterSSE(EEREC_D), xRegisterSSE(t1reg));
+			xPMOVSX.DQ(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_D));
+		}
+		else
+		{
+			xPUNPCK.LDQ(xRegisterSSE(EEREC_D), xRegisterSSE(t1reg));
+			xMOVDQA(xRegisterSSE(t0reg), xRegisterSSE(EEREC_D));
+			xPSRA.D(xRegisterSSE(t0reg), 31); // get the signs
+			xPUNPCK.LDQ(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg));
+		}
 
 		_freeXMMreg(t0reg);
 		_freeXMMreg(t1reg);
@@ -2003,6 +2180,12 @@ void recPMSUBW()
 {
 	EE::Profiler.EmitOp(eeOpcode::PMSUBW);
 
+	if( !x86caps.hasStreamingSIMD4Extensions ) {
+		_deleteEEreg(_Rd_, 0);
+		recCall(Interp::PMSUBW);
+		return;
+	}
+
 	int info = eeRecompileCodeXMM((((_Rs_) && (_Rt_)) ? XMMINFO_READS : 0) | (((_Rs_) && (_Rt_)) ? XMMINFO_READT : 0) | (_Rd_ ? XMMINFO_WRITED : 0) | XMMINFO_WRITELO | XMMINFO_WRITEHI | XMMINFO_READLO | XMMINFO_READHI);
 	xSHUF.PS(xRegisterSSE(EEREC_LO), xRegisterSSE(EEREC_HI), 0x88);
 	xPSHUF.D(xRegisterSSE(EEREC_LO), xRegisterSSE(EEREC_LO), 0xd8); // LO = {LO[0], HI[0], LO[2], HI[2]}
@@ -2064,6 +2247,12 @@ void recPMULTW()
 {
 	EE::Profiler.EmitOp(eeOpcode::PMULTW);
 
+	if( !x86caps.hasStreamingSIMD4Extensions ) {
+		_deleteEEreg(_Rd_, 0);
+		recCall(Interp::PMULTW);
+		return;
+	}
+
 	int info = eeRecompileCodeXMM((((_Rs_) && (_Rt_)) ? XMMINFO_READS : 0) | (((_Rs_) && (_Rt_)) ? XMMINFO_READT : 0) | (_Rd_ ? XMMINFO_WRITED : 0) | XMMINFO_WRITELO | XMMINFO_WRITEHI);
 	if (!_Rs_ || !_Rt_)
 	{
@@ -2607,8 +2796,20 @@ void recPSRAVW()
 		}
 		else
 		{
-			xPSHUF.D(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T), 0x88);
-			xPMOVSX.DQ(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_D));
+			if ( x86caps.hasStreamingSIMD4Extensions )
+			{
+				xPSHUF.D(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T), 0x88);
+				xPMOVSX.DQ(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_D));
+			}
+			else
+			{
+				int t0reg = _allocTempXMMreg(XMMT_INT, -1);
+				xPSHUF.D(xRegisterSSE(EEREC_D), xRegisterSSE(EEREC_T), 0x88);
+				xMOVDQA(xRegisterSSE(t0reg), xRegisterSSE(EEREC_D));
+				xPSRA.D(xRegisterSSE(t0reg), 31);
+				xPUNPCK.LDQ(xRegisterSSE(EEREC_D), xRegisterSSE(t0reg));
+				_freeXMMreg(t0reg);
+			}
 		}
 	}
 	else if (_Rt_ == 0)
diff --git a/pcsx2/x86/ix86-32/iR5900-32.cpp b/pcsx2/x86/ix86-32/iR5900-32.cpp
index 3cb12f9..bf7b8d5 100644
--- a/pcsx2/x86/ix86-32/iR5900-32.cpp
+++ b/pcsx2/x86/ix86-32/iR5900-32.cpp
@@ -551,8 +551,8 @@ static void recReserve()
 {
 	// Hardware Requirements Check...
 
-	if (!x86caps.hasStreamingSIMD4Extensions)
-		recThrowHardwareDeficiency(L"SSE4");
+	if (!x86caps.hasStreamingSIMD2Extensions)
+		recThrowHardwareDeficiency(L"SSE2");
 
 	recReserveCache();
 }
diff --git a/pcsx2/x86/microVU.cpp b/pcsx2/x86/microVU.cpp
index ba419f0..b3a8d65 100644
--- a/pcsx2/x86/microVU.cpp
+++ b/pcsx2/x86/microVU.cpp
@@ -50,8 +50,8 @@ void mVUreserveCache(microVU& mVU)
 void mVUinit(microVU& mVU, uint vuIndex)
 {
 
-	if (!x86caps.hasStreamingSIMD4Extensions)
-		mVUthrowHardwareDeficiency(L"SSE4", vuIndex);
+	if (!x86caps.hasStreamingSIMD2Extensions)
+		mVUthrowHardwareDeficiency(L"SSE2", vuIndex);
 
 	memzero(mVU.prog);
 
diff --git a/pcsx2/x86/microVU_Alloc.inl b/pcsx2/x86/microVU_Alloc.inl
index 9ce1e4e..cd2b53b 100644
--- a/pcsx2/x86/microVU_Alloc.inl
+++ b/pcsx2/x86/microVU_Alloc.inl
@@ -171,7 +171,16 @@ __fi void getQreg(const xmm& reg, int qInstance)
 __ri void writeQreg(const xmm& reg, int qInstance)
 {
 	if (qInstance)
-		xINSERTPS(xmmPQ, reg, _MM_MK_INSERTPS_NDX(0, 1, 0));
+	{
+		if (!x86caps.hasStreamingSIMD4Extensions)
+		{
+			xPSHUF.D(xmmPQ, xmmPQ, 0xe1);
+			xMOVSS(xmmPQ, reg);
+			xPSHUF.D(xmmPQ, xmmPQ, 0xe1);
+		}
+		else
+			xINSERTPS(xmmPQ, reg, _MM_MK_INSERTPS_NDX(0, 1, 0));
+	}
 	else
 		xMOVSS(xmmPQ, reg);
 }
diff --git a/pcsx2/x86/microVU_Clamp.inl b/pcsx2/x86/microVU_Clamp.inl
index 5184a0c..c9bdfc9 100644
--- a/pcsx2/x86/microVU_Clamp.inl
+++ b/pcsx2/x86/microVU_Clamp.inl
@@ -61,10 +61,34 @@ void mVUclamp2(microVU& mVU, const xmm& reg, const xmm& regT1in, int xyzw, bool
 {
 	if ((!clampE && CHECK_VU_SIGN_OVERFLOW) || (clampE && bClampE && CHECK_VU_SIGN_OVERFLOW))
 	{
-		int i = (xyzw == 1 || xyzw == 2 || xyzw == 4 || xyzw == 8) ? 0 : 1;
-		xPMIN.SD(reg, ptr128[&sse4_maxvals[i][0]]);
-		xPMIN.UD(reg, ptr128[&sse4_minvals[i][0]]);
-		return;
+		if (x86caps.hasStreamingSIMD4Extensions)
+		{
+			int i = (xyzw == 1 || xyzw == 2 || xyzw == 4 || xyzw == 8) ? 0 : 1;
+			xPMIN.SD(reg, ptr128[&sse4_maxvals[i][0]]);
+			xPMIN.UD(reg, ptr128[&sse4_minvals[i][0]]);
+			return;
+		}
+		//const xmm& regT1 = regT1b ? mVU.regAlloc->allocReg() : regT1in;
+		const xmm& regT1 = regT1in.IsEmpty() ? xmm((reg.Id + 1) % 8) : regT1in;
+		if (regT1 != regT1in) xMOVAPS(ptr128[mVU.xmmCTemp], regT1);
+		switch (xyzw) {
+			case 1: case 2: case 4: case 8:
+				xMOVAPS(regT1, reg);
+				xAND.PS(regT1, ptr128[mVUglob.signbit]);
+				xMIN.SS(reg,   ptr128[mVUglob.maxvals]);
+				xMAX.SS(reg,   ptr128[mVUglob.minvals]);
+				xOR.PS (reg,   regT1);
+				break;
+			default:
+				xMOVAPS(regT1, reg);
+				xAND.PS(regT1, ptr128[mVUglob.signbit]);
+				xMIN.PS(reg,   ptr128[mVUglob.maxvals]);
+				xMAX.PS(reg,   ptr128[mVUglob.minvals]);
+				xOR.PS (reg,   regT1);
+				break;
+		}
+		//if (regT1 != regT1in) mVU.regAlloc->clearNeeded(regT1);
+		if (regT1 != regT1in) xMOVAPS(regT1, ptr128[mVU.xmmCTemp]);
 	}
 	else
 		mVUclamp1(reg, regT1in, xyzw, bClampE);
diff --git a/pcsx2/x86/microVU_Lower.inl b/pcsx2/x86/microVU_Lower.inl
index 45fd825..e87fba1 100644
--- a/pcsx2/x86/microVU_Lower.inl
+++ b/pcsx2/x86/microVU_Lower.inl
@@ -28,7 +28,12 @@ static __fi void testZero(const xmm& xmmReg, const xmm& xmmTemp, const x32& gprT
 {
 	xXOR.PS(xmmTemp, xmmTemp);
 	xCMPEQ.SS(xmmTemp, xmmReg);
-	xPTEST(xmmTemp, xmmTemp);
+	if (!x86caps.hasStreamingSIMD4Extensions)
+	{
+		xMOVMSKPS(gprTemp, xmmTemp);
+		xTEST(gprTemp, 1);
+	}
+	else xPTEST(xmmTemp, xmmTemp);
 }
 
 // Test if Vector is Negative (Set Flags and Makes Positive)
@@ -345,8 +350,20 @@ mVUop(mVU_EEXP)
 // sumXYZ(): PQ.x = x ^ 2 + y ^ 2 + z ^ 2
 static __fi void mVU_sumXYZ(mV, const xmm& PQ, const xmm& Fs)
 {
-	xDP.PS(Fs, Fs, 0x71);
-	xMOVSS(PQ, Fs);
+	if (x86caps.hasStreamingSIMD4Extensions)
+	{
+		xDP.PS(Fs, Fs, 0x71);
+		xMOVSS(PQ, Fs);
+	}
+	else
+	{
+		SSE_MULPS(mVU, Fs, Fs);	   // wzyx ^ 2
+		xMOVSS		(PQ, Fs);		  // x ^ 2
+		xPSHUF.D	  (Fs, Fs, 0xe1); // wzyx -> wzxy
+		SSE_ADDSS(mVU, PQ, Fs);	   // x ^ 2 + y ^ 2
+		xPSHUF.D	  (Fs, Fs, 0xd2); // wzxy -> wxyz
+		SSE_ADDSS(mVU, PQ, Fs);	   // x ^ 2 + y ^ 2 + z ^ 2
+	}
 }
 
 mVUop(mVU_ELENG)
diff --git a/pcsx2/x86/microVU_Misc.inl b/pcsx2/x86/microVU_Misc.inl
index 8db7f62..e77ded2 100644
--- a/pcsx2/x86/microVU_Misc.inl
+++ b/pcsx2/x86/microVU_Misc.inl
@@ -55,36 +55,90 @@ void mVUsaveReg(const xmm& reg, xAddressVoid ptr, int xyzw, bool modXYZW)
 	switch (xyzw)
 	{
 		case 5: // YW
-			xEXTRACTPS(ptr32[ptr + 4], reg, 1);
-			xEXTRACTPS(ptr32[ptr + 12], reg, 3);
+			if (x86caps.hasStreamingSIMD4Extensions)
+			{
+				xEXTRACTPS(ptr32[ptr + 4], reg, 1);
+				xEXTRACTPS(ptr32[ptr + 12], reg, 3);
+			}
+			else
+			{
+				xPSHUF.D(reg, reg, 0xe1); //WZXY
+				xMOVSS(ptr32[ptr + 4], reg);
+				xPSHUF.D(reg, reg, 0xff); //WWWW
+				xMOVSS(ptr32[ptr + 12], reg);
+			}
 			break;
 		case 6: // YZ
 			xPSHUF.D(reg, reg, 0xc9);
 			xMOVL.PS(ptr64[ptr + 4], reg);
 			break;
 		case 7: // YZW
-			xMOVH.PS(ptr64[ptr + 8], reg);
-			xEXTRACTPS(ptr32[ptr + 4], reg, 1);
+			if (x86caps.hasStreamingSIMD4Extensions)
+			{
+				xMOVH.PS(ptr64[ptr + 8], reg);
+				xEXTRACTPS(ptr32[ptr + 4],  reg, 1);
+			}
+			else
+			{
+				xPSHUF.D(reg, reg, 0x93); //ZYXW
+				xMOVH.PS(ptr64[ptr + 4], reg);
+				xMOVSS(ptr32[ptr + 12], reg);
+			}
 			break;
 		case 9: // XW
-			xMOVSS(ptr32[ptr], reg);
-			xEXTRACTPS(ptr32[ptr + 12], reg, 3);
+			if (x86caps.hasStreamingSIMD4Extensions)
+			{
+				xMOVSS(ptr32[ptr], reg);
+				xEXTRACTPS(ptr32[ptr + 12], reg, 3);
+			}
+			else
+			{
+				xMOVSS(ptr32[ptr], reg);
+				xPSHUF.D(reg, reg, 0xff); //WWWW
+				xMOVSS(ptr32[ptr + 12], reg);
+			}
 			break;
 		case 10: // XZ
-			xMOVSS(ptr32[ptr], reg);
-			xEXTRACTPS(ptr32[ptr + 8], reg, 2);
+			if (x86caps.hasStreamingSIMD4Extensions)
+			{
+				xMOVSS(ptr32[ptr], reg);
+				xEXTRACTPS(ptr32[ptr + 8], reg, 2);
+			}
+			else {
+				xMOVSS(ptr32[ptr], reg);
+				xMOVHL.PS(reg, reg);
+				xMOVSS(ptr32[ptr + 8], reg);
+			}
 			break;
 		case 11: // XZW
 			xMOVSS(ptr32[ptr], reg);
 			xMOVH.PS(ptr64[ptr + 8], reg);
 			break;
 		case 13: // XYW
-			xMOVL.PS(ptr64[ptr], reg);
-			xEXTRACTPS(ptr32[ptr + 12], reg, 3);
+			if (x86caps.hasStreamingSIMD4Extensions)
+			{
+				xMOVL.PS(ptr64[ptr], reg);
+				xEXTRACTPS(ptr32[ptr+12], reg, 3);
+			}
+			else
+			{
+				xPSHUF.D(reg, reg, 0x4b); //YXZW
+				xMOVH.PS(ptr64[ptr], reg);
+				xMOVSS(ptr32[ptr+12], reg);
+			}
 			break;
 		case 14: // XYZ
-			xMOVL.PS(ptr64[ptr], reg);
-			xEXTRACTPS(ptr32[ptr + 8], reg, 2);
+			if (x86caps.hasStreamingSIMD4Extensions)
+			{
+				xMOVL.PS(ptr64[ptr], reg);
+				xEXTRACTPS(ptr32[ptr+8], reg, 2);
+			}
+			else
+			{
+				xMOVL.PS(ptr64[ptr], reg);
+				xMOVHL.PS(reg, reg);
+				xMOVSS(ptr32[ptr+8], reg);
+			}
 			break;
 		case 4: // Y
 			if (!modXYZW)
@@ -120,14 +174,9 @@ void mVUsaveReg(const xmm& reg, xAddressVoid ptr, int xyzw, bool modXYZW)
 void mVUmergeRegs(const xmm& dest, const xmm& src, int xyzw, bool modXYZW)
 {
 	xyzw &= 0xf;
-	if ((dest != src) && (xyzw != 0))
+	if ( (dest != src) && (xyzw != 0) )
 	{
-		if (xyzw == 0x8)
-			xMOVSS(dest, src);
-		else if (xyzw == 0xf)
-			xMOVAPS(dest, src);
-		else
-		{
+		if (x86caps.hasStreamingSIMD4Extensions && (xyzw != 0x8) && (xyzw != 0xf)) {
 			if (modXYZW)
 			{
 				if      (xyzw == 1) { xINSERTPS(dest, src, _MM_MK_INSERTPS_NDX(0, 3, 0)); return; }
@@ -137,6 +186,57 @@ void mVUmergeRegs(const xmm& dest, const xmm& src, int xyzw, bool modXYZW)
 			xyzw = ((xyzw & 1) << 3) | ((xyzw & 2) << 1) | ((xyzw & 4) >> 1) | ((xyzw & 8) >> 3);
 			xBLEND.PS(dest, src, xyzw);
 		}
+		else
+		{
+			switch (xyzw) {
+				case 1:  if (modXYZW) mVUunpack_xyzw(src, src, 0);
+						 xMOVHL.PS(src, dest);		 // src = Sw Sz Dw Dz
+						 xSHUF.PS(dest, src, 0xc4); // 11 00 01 00
+						 break;
+				case 2:  if (modXYZW) mVUunpack_xyzw(src, src, 0);
+						 xMOVHL.PS(src, dest);
+						 xSHUF.PS(dest, src, 0x64);
+						 break;
+				case 3:	 xSHUF.PS(dest, src, 0xe4);
+						 break;
+				case 4:	 if (modXYZW) mVUunpack_xyzw(src, src, 0);
+						 xMOVSS(src, dest);
+						 xMOVSD(dest, src);
+						 break;
+				case 5:	 xSHUF.PS(dest, src, 0xd8);
+						 xPSHUF.D(dest, dest, 0xd8);
+						 break;
+				case 6:	 xSHUF.PS(dest, src, 0x9c);
+						 xPSHUF.D(dest, dest, 0x78);
+						 break;
+				case 7:	 xMOVSS(src, dest);
+						 xMOVAPS(dest, src);
+						 break;
+				case 8:	 xMOVSS(dest, src);
+						 break;
+				case 9:	 xSHUF.PS(dest, src, 0xc9);
+						 xPSHUF.D(dest, dest, 0xd2);
+						 break;
+				case 10: xSHUF.PS(dest, src, 0x8d);
+						 xPSHUF.D(dest, dest, 0x72);
+						 break;
+				case 11: xMOVSS(dest, src);
+						 xSHUF.PS(dest, src, 0xe4);
+						 break;
+				case 12: xMOVSD(dest, src);
+						 break;
+				case 13: xMOVHL.PS(dest, src);
+						 xSHUF.PS(src, dest, 0x64);
+						 xMOVAPS(dest, src);
+						 break;
+				case 14: xMOVHL.PS(dest, src);
+						 xSHUF.PS(src, dest, 0xc4);
+						 xMOVAPS(dest, src);
+						 break;
+				default: xMOVAPS(dest, src);
+						 break;
+			}
+		}
 	}
 }
 
diff --git a/pcsx2/x86/microVU_Upper.inl b/pcsx2/x86/microVU_Upper.inl
index f993eb4..d280ce9 100644
--- a/pcsx2/x86/microVU_Upper.inl
+++ b/pcsx2/x86/microVU_Upper.inl
@@ -30,11 +30,6 @@
 	} while (0)
 
 
-alignas(16) const u32 sse4_compvals[2][4] = {
-	{0x7f7fffff, 0x7f7fffff, 0x7f7fffff, 0x7f7fffff}, //1111
-	{0x7fffffff, 0x7fffffff, 0x7fffffff, 0x7fffffff}, //1111
-};
-
 // Note: If modXYZW is true, then it adjusts XYZW for Single Scalar operations
 static void mVUupdateFlags(mV, const xmm& reg, const xmm& regT1in = xEmptyReg, const xmm& regT2in = xEmptyReg, bool modXYZW = 1)
 {
@@ -78,41 +73,18 @@ static void mVUupdateFlags(mV, const xmm& reg, const xmm& regT1in = xEmptyReg, c
 	xMOVMSKPS(gprT2, regT1); // Used for Zero Flag Calculation
 
 	xAND(mReg, AND_XYZW); // Grab "Is Signed" bits from the previous calculation
-	xSHL(mReg, 4);
+	xSHL(mReg, 4 + ADD_XYZW);
 
 	//-------------------------Check for Zero flags------------------------------
 
 	xAND(gprT2, AND_XYZW); // Grab "Is Zero" bits from the previous calculation
+	if (mFLAG.doFlag)
+		SHIFT_XYZW(gprT2);
 	xOR(mReg, gprT2);
 
-	//-------------------------Overflow Flags-----------------------------------
-	// We can't really do this because of the limited range of x86 and the value MIGHT genuinely be FLT_MAX (x86)
-	// so this will need to remain as a gamefix for the one game that needs it (Superman Returns)
-	// until some sort of soft float implementation.
-	if (sFLAG.doFlag && CHECK_VUOVERFLOWHACK)
-	{
-		//Calculate overflow
-		xMOVAPS(regT1, regT2);
-		xAND.PS(regT1, ptr128[&sse4_compvals[1][0]]); // Remove sign flags (we don't care)
-		xCMPNLT.PS(regT1, ptr128[&sse4_compvals[0][0]]); // Compare if T1 == FLT_MAX
-		xMOVMSKPS(gprT2, regT1); // Grab sign bits  for equal results
-		xAND(gprT2, AND_XYZW); // Grab "Is FLT_MAX" bits from the previous calculation
-		xForwardJump32 oJMP(Jcc_Zero);
-
-		xOR(sReg, 0x820000);
-		if (mFLAG.doFlag)
-		{
-			xSHL(gprT2, 12); // Add the results to the MAC Flag
-			xOR(mReg, gprT2);
-		}
-
-		oJMP.SetTarget();
-	}
-
 	//-------------------------Write back flags------------------------------
 	if (mFLAG.doFlag)
 	{
-		SHIFT_XYZW(mReg); // If it was Single Scalar, move the flags in to the correct position
 		mVUallocMFLAGb(mVU, mReg, mFLAG.write); // Set Mac Flag
 	}
 	if (sFLAG.doFlag)
diff --git a/pcsx2/x86/newVif_UnpackSSE.cpp b/pcsx2/x86/newVif_UnpackSSE.cpp
index cfcceeb..f6adb27 100644
--- a/pcsx2/x86/newVif_UnpackSSE.cpp
+++ b/pcsx2/x86/newVif_UnpackSSE.cpp
@@ -36,7 +36,23 @@ static RecompiledCodeReserve* nVifUpkExec = NULL;
 // Merges xmm vectors without modifying source reg
 void mergeVectors(xRegisterSSE dest, xRegisterSSE src, xRegisterSSE temp, int xyzw)
 {
-	mVUmergeRegs(dest, src, xyzw);
+	if (x86caps.hasStreamingSIMD4Extensions || (xyzw==15)
+		|| (xyzw==12) || (xyzw==11) || (xyzw==8) || (xyzw==3)) {
+			mVUmergeRegs(dest, src, xyzw);
+	}
+	else
+	{
+		if(temp != src) xMOVAPS(temp, src); //Sometimes we don't care if the source is modified and is temp reg.
+		if(dest == temp)
+		{
+			//VIF can sent the temp directory as the source and destination, just need to clear the ones we dont want in which case.
+			if(!(xyzw & 0x1)) xAND.PS( dest, ptr128[SSEXYZWMask[0]]);
+			if(!(xyzw & 0x2)) xAND.PS( dest, ptr128[SSEXYZWMask[1]]);
+			if(!(xyzw & 0x4)) xAND.PS( dest, ptr128[SSEXYZWMask[2]]);
+			if(!(xyzw & 0x8)) xAND.PS( dest, ptr128[SSEXYZWMask[3]]);
+		}
+		else mVUmergeRegs(dest, temp, xyzw);
+	}
 }
 
 // =====================================================================================================
@@ -103,6 +119,16 @@ void VifUnpackSSE_Base::xUPK_S_32() const
 void VifUnpackSSE_Base::xUPK_S_16() const
 {
 
+	if (!x86caps.hasStreamingSIMD4Extensions)
+	{
+			xMOV16     (workReg, ptr32[srcIndirect]);
+			xPUNPCK.LWD(workReg, workReg);
+			xShiftR    (workReg, 16);
+
+			xPSHUF.D   (destReg, workReg, _v0);
+			return;
+	}
+
 	switch (UnpkLoopIteration)
 	{
 		case 0:
@@ -124,6 +150,17 @@ void VifUnpackSSE_Base::xUPK_S_16() const
 void VifUnpackSSE_Base::xUPK_S_8() const
 {
 
+	if (!x86caps.hasStreamingSIMD4Extensions)
+	{
+		xMOV8      (workReg, ptr32[srcIndirect]);
+		xPUNPCK.LBW(workReg, workReg);
+		xPUNPCK.LWD(workReg, workReg);
+		xShiftR    (workReg, 24);
+
+		xPSHUF.D   (destReg, workReg, _v0);
+		return;
+	}
+
 	switch (UnpkLoopIteration)
 	{
 		case 0:
@@ -170,7 +207,16 @@ void VifUnpackSSE_Base::xUPK_V2_16() const
 
 	if (UnpkLoopIteration == 0)
 	{
-		xPMOVXX16(workReg);
+		if (x86caps.hasStreamingSIMD4Extensions)
+		{
+				xPMOVXX16(workReg);
+		}
+		else
+		{
+				xMOV64     (workReg, ptr64[srcIndirect]);
+				xPUNPCK.LWD(workReg, workReg);
+				xShiftR    (workReg, 16);
+		}
 		xPSHUF.D(destReg, workReg, 0x44); //v1v0v1v0
 	}
 	else
@@ -182,9 +228,19 @@ void VifUnpackSSE_Base::xUPK_V2_16() const
 void VifUnpackSSE_Base::xUPK_V2_8() const
 {
 
-	if (UnpkLoopIteration == 0)
+	if (UnpkLoopIteration == 0 || !x86caps.hasStreamingSIMD4Extensions)
 	{
-		xPMOVXX8(workReg);
+		if (x86caps.hasStreamingSIMD4Extensions)
+		{
+			xPMOVXX8   (workReg);
+		}
+		else
+		{
+			xMOV16     (workReg, ptr32[srcIndirect]);
+			xPUNPCK.LBW(workReg, workReg);
+			xPUNPCK.LWD(workReg, workReg);
+			xShiftR    (workReg, 24);
+		}
 		xPSHUF.D(destReg, workReg, 0x44); //v1v0v1v0
 	}
 	else
@@ -204,7 +260,16 @@ void VifUnpackSSE_Base::xUPK_V3_32() const
 void VifUnpackSSE_Base::xUPK_V3_16() const
 {
 
-	xPMOVXX16(destReg);
+	if (x86caps.hasStreamingSIMD4Extensions)
+	{
+		xPMOVXX16  (destReg);
+	}
+	else
+	{
+		xMOV64     (destReg, ptr32[srcIndirect]);
+		xPUNPCK.LWD(destReg, destReg);
+		xShiftR    (destReg, 16);
+	}
 
 	//With V3-16, it takes the first vector from the next position as the W vector
 	//However - IF the end of this iteration of the unpack falls on a quadword boundary, W becomes 0
@@ -221,7 +286,17 @@ void VifUnpackSSE_Base::xUPK_V3_16() const
 void VifUnpackSSE_Base::xUPK_V3_8() const
 {
 
-	xPMOVXX8(destReg);
+	if (x86caps.hasStreamingSIMD4Extensions)
+	{
+		xPMOVXX8   (destReg);
+	}
+	else
+	{
+		xMOV32     (destReg, ptr32[srcIndirect]);
+		xPUNPCK.LBW(destReg, destReg);
+		xPUNPCK.LWD(destReg, destReg);
+		xShiftR    (destReg, 24);
+	}
 	if (UnpkLoopIteration != IsAligned)
 		xAND.PS(destReg, ptr128[SSEXYZWMask[0]]);
 }
@@ -233,12 +308,31 @@ void VifUnpackSSE_Base::xUPK_V4_32() const
 
 void VifUnpackSSE_Base::xUPK_V4_16() const
 {
-	xPMOVXX16(destReg);
+	if (x86caps.hasStreamingSIMD4Extensions)
+	{
+		xPMOVXX16  (destReg);
+	}
+	else
+	{
+		xMOV64     (destReg, ptr32[srcIndirect]);
+		xPUNPCK.LWD(destReg, destReg);
+		xShiftR    (destReg, 16);
+	}
 }
 
 void VifUnpackSSE_Base::xUPK_V4_8() const
 {
-	xPMOVXX8(destReg);
+	if (x86caps.hasStreamingSIMD4Extensions)
+	{
+		xPMOVXX8   (destReg);
+	}
+	else
+	{
+		xMOV32     (destReg, ptr32[srcIndirect]);
+		xPUNPCK.LBW(destReg, destReg);
+		xPUNPCK.LWD(destReg, destReg);
+		xShiftR    (destReg, 24);
+	}
 }
 
 void VifUnpackSSE_Base::xUPK_V4_5() const
-- 
2.34.1

